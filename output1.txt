Loading model from models/ldm/text2img-large/model.ckpt
autoencoder.py
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 872.30 M params.
ldm/modules/diffusionmodules/model.py, Enocoder; in_channels = 3, ch_mult = [1, 2, 4, 4]
ldm/modules/diffusionmodules/model.py, Resnet in_channels != out_channels
ldm/modules/diffusionmodules/model.py, Resnet in_channels != out_channels
making attention of type 'vanilla' with 512 in_channels
ldm/models/autoencoder.py, AutoencoderKL init, created
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
ldm/modules/diffusionmodules/model.py, Resnet in_channels != out_channels
ldm/modules/diffusionmodules/model.py, Resnet in_channels != out_channels
ddpm.py instantiate_first_stage = AutoencoderKL(
  (encoder): Encoder(
    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (down): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0-1): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (1): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (2): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (3): Module(
        (block): ModuleList(
          (0-1): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
    )
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): AttnBlock(
        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (decoder): Decoder(
    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): AttnBlock(
        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (up): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
      (1): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2-3): 2 x Module(
        (block): ModuleList(
          (0-2): 3 x ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (loss): Identity()
  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))
  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))
)
remove_png complete
img shape = torch.Size([2, 3, 256, 256])
images are saved in outputs/uinput.png
ldm.models.diffusion.ddpm.py, first_stage_model = AutoencoderKL(
  (encoder): Encoder(
    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (down): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0-1): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (1): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (2): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (3): Module(
        (block): ModuleList(
          (0-1): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
    )
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): AttnBlock(
        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (decoder): Decoder(
    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): AttnBlock(
        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (up): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
      (1): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2-3): 2 x Module(
        (block): ModuleList(
          (0-2): 3 x ResnetBlock(
            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (loss): Identity()
  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))
  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))
)
Encoder, forward@@@@@@@@@@@@@@@
autoencoder.py, h = torch.Size([2, 8, 32, 32])
autoencoder.py , moments = torch.Size([2, 8, 32, 32])
encode start = 
z = torch.Size([2, 4, 32, 32]), z_max = 3.3438422679901123, z_min = -3.334462881088257
images are saved in outputs/z.png
images are saved in outputs/z_0.png
images are saved in outputs/nosample_0.png
####cond finisihed #####
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps, Selected timesteps for ddim sampler: (200,)
ldm/modules/diffusionmodules/util.py, make_ddim_sampling_parameters, Selected alphas for ddim sampler: a_t: torch.Size([200]); a_(t-1): (200,)
For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)
ddim.py alphas_cumprod = tensor([0.9991, 0.9983, 0.9974, 0.9966, 0.9957, 0.9948, 0.9940, 0.9931, 0.9922,
        0.9913, 0.9904, 0.9895, 0.9886, 0.9877, 0.9868, 0.9859, 0.9850, 0.9841,
        0.9832, 0.9822, 0.9813, 0.9804, 0.9794, 0.9785, 0.9776, 0.9766, 0.9757,
        0.9747, 0.9737, 0.9728, 0.9718, 0.9708, 0.9698, 0.9689, 0.9679, 0.9669,
        0.9659, 0.9649, 0.9639, 0.9629, 0.9619, 0.9609, 0.9599, 0.9588, 0.9578,
        0.9568, 0.9557, 0.9547, 0.9537, 0.9526, 0.9516, 0.9505, 0.9495, 0.9484,
        0.9473, 0.9463, 0.9452, 0.9441, 0.9430, 0.9420, 0.9409, 0.9398, 0.9387,
        0.9376, 0.9365, 0.9354, 0.9343, 0.9332, 0.9320, 0.9309, 0.9298, 0.9287,
        0.9275, 0.9264, 0.9252, 0.9241, 0.9229, 0.9218, 0.9206, 0.9195, 0.9183,
        0.9171, 0.9160, 0.9148, 0.9136, 0.9124, 0.9112, 0.9100, 0.9089, 0.9077,
        0.9065, 0.9052, 0.9040, 0.9028, 0.9016, 0.9004, 0.8992, 0.8979, 0.8967,
        0.8955, 0.8942, 0.8930, 0.8917, 0.8905, 0.8892, 0.8880, 0.8867, 0.8854,
        0.8842, 0.8829, 0.8816, 0.8804, 0.8791, 0.8778, 0.8765, 0.8752, 0.8739,
        0.8726, 0.8713, 0.8700, 0.8687, 0.8674, 0.8661, 0.8647, 0.8634, 0.8621,
        0.8607, 0.8594, 0.8581, 0.8567, 0.8554, 0.8540, 0.8527, 0.8513, 0.8500,
        0.8486, 0.8473, 0.8459, 0.8445, 0.8431, 0.8418, 0.8404, 0.8390, 0.8376,
        0.8362, 0.8348, 0.8334, 0.8320, 0.8306, 0.8292, 0.8278, 0.8264, 0.8250,
        0.8236, 0.8221, 0.8207, 0.8193, 0.8179, 0.8164, 0.8150, 0.8136, 0.8121,
        0.8107, 0.8092, 0.8078, 0.8063, 0.8049, 0.8034, 0.8019, 0.8005, 0.7990,
        0.7975, 0.7960, 0.7946, 0.7931, 0.7916, 0.7901, 0.7886, 0.7871, 0.7856,
        0.7842, 0.7827, 0.7812, 0.7796, 0.7781, 0.7766, 0.7751, 0.7736, 0.7721,
        0.7706, 0.7690, 0.7675, 0.7660, 0.7645, 0.7629, 0.7614, 0.7599, 0.7583,
        0.7568, 0.7552, 0.7537, 0.7521, 0.7506, 0.7490, 0.7475, 0.7459, 0.7444,
        0.7428, 0.7412, 0.7397, 0.7381, 0.7365, 0.7350, 0.7334, 0.7318, 0.7302,
        0.7286, 0.7271, 0.7255, 0.7239, 0.7223, 0.7207, 0.7191, 0.7175, 0.7159,
        0.7143, 0.7127, 0.7111, 0.7095, 0.7079, 0.7063, 0.7047, 0.7031, 0.7015,
        0.6999, 0.6982, 0.6966, 0.6950, 0.6934, 0.6918, 0.6901, 0.6885, 0.6869,
        0.6852, 0.6836, 0.6820, 0.6803, 0.6787, 0.6771, 0.6754, 0.6738, 0.6722,
        0.6705, 0.6689, 0.6672, 0.6656, 0.6639, 0.6623, 0.6606, 0.6590, 0.6573,
        0.6557, 0.6540, 0.6524, 0.6507, 0.6490, 0.6474, 0.6457, 0.6441, 0.6424,
        0.6407, 0.6391, 0.6374, 0.6357, 0.6341, 0.6324, 0.6307, 0.6291, 0.6274,
        0.6257, 0.6241, 0.6224, 0.6207, 0.6190, 0.6174, 0.6157, 0.6140, 0.6123,
        0.6107, 0.6090, 0.6073, 0.6056, 0.6039, 0.6023, 0.6006, 0.5989, 0.5972,
        0.5955, 0.5939, 0.5922, 0.5905, 0.5888, 0.5871, 0.5855, 0.5838, 0.5821,
        0.5804, 0.5787, 0.5770, 0.5754, 0.5737, 0.5720, 0.5703, 0.5686, 0.5669,
        0.5652, 0.5636, 0.5619, 0.5602, 0.5585, 0.5568, 0.5551, 0.5535, 0.5518,
        0.5501, 0.5484, 0.5467, 0.5450, 0.5434, 0.5417, 0.5400, 0.5383, 0.5366,
        0.5350, 0.5333, 0.5316, 0.5299, 0.5282, 0.5266, 0.5249, 0.5232, 0.5215,
        0.5199, 0.5182, 0.5165, 0.5148, 0.5132, 0.5115, 0.5098, 0.5082, 0.5065,
        0.5048, 0.5032, 0.5015, 0.4998, 0.4982, 0.4965, 0.4948, 0.4932, 0.4915,
        0.4898, 0.4882, 0.4865, 0.4849, 0.4832, 0.4816, 0.4799, 0.4782, 0.4766,
        0.4749, 0.4733, 0.4716, 0.4700, 0.4684, 0.4667, 0.4651, 0.4634, 0.4618,
        0.4601, 0.4585, 0.4569, 0.4552, 0.4536, 0.4520, 0.4503, 0.4487, 0.4471,
        0.4455, 0.4438, 0.4422, 0.4406, 0.4390, 0.4374, 0.4357, 0.4341, 0.4325,
        0.4309, 0.4293, 0.4277, 0.4261, 0.4245, 0.4229, 0.4213, 0.4197, 0.4181,
        0.4165, 0.4149, 0.4133, 0.4117, 0.4101, 0.4086, 0.4070, 0.4054, 0.4038,
        0.4022, 0.4007, 0.3991, 0.3975, 0.3960, 0.3944, 0.3928, 0.3913, 0.3897,
        0.3882, 0.3866, 0.3850, 0.3835, 0.3819, 0.3804, 0.3789, 0.3773, 0.3758,
        0.3742, 0.3727, 0.3712, 0.3697, 0.3681, 0.3666, 0.3651, 0.3636, 0.3621,
        0.3605, 0.3590, 0.3575, 0.3560, 0.3545, 0.3530, 0.3515, 0.3500, 0.3485,
        0.3470, 0.3456, 0.3441, 0.3426, 0.3411, 0.3396, 0.3382, 0.3367, 0.3352,
        0.3338, 0.3323, 0.3308, 0.3294, 0.3279, 0.3265, 0.3250, 0.3236, 0.3222,
        0.3207, 0.3193, 0.3178, 0.3164, 0.3150, 0.3136, 0.3122, 0.3107, 0.3093,
        0.3079, 0.3065, 0.3051, 0.3037, 0.3023, 0.3009, 0.2995, 0.2981, 0.2967,
        0.2954, 0.2940, 0.2926, 0.2912, 0.2899, 0.2885, 0.2871, 0.2858, 0.2844,
        0.2831, 0.2817, 0.2804, 0.2790, 0.2777, 0.2763, 0.2750, 0.2737, 0.2723,
        0.2710, 0.2697, 0.2684, 0.2671, 0.2658, 0.2645, 0.2631, 0.2618, 0.2606,
        0.2593, 0.2580, 0.2567, 0.2554, 0.2541, 0.2528, 0.2516, 0.2503, 0.2490,
        0.2478, 0.2465, 0.2453, 0.2440, 0.2428, 0.2415, 0.2403, 0.2391, 0.2378,
        0.2366, 0.2354, 0.2341, 0.2329, 0.2317, 0.2305, 0.2293, 0.2281, 0.2269,
        0.2257, 0.2245, 0.2233, 0.2221, 0.2209, 0.2198, 0.2186, 0.2174, 0.2163,
        0.2151, 0.2139, 0.2128, 0.2116, 0.2105, 0.2093, 0.2082, 0.2071, 0.2059,
        0.2048, 0.2037, 0.2026, 0.2014, 0.2003, 0.1992, 0.1981, 0.1970, 0.1959,
        0.1948, 0.1937, 0.1926, 0.1915, 0.1905, 0.1894, 0.1883, 0.1872, 0.1862,
        0.1851, 0.1841, 0.1830, 0.1820, 0.1809, 0.1799, 0.1788, 0.1778, 0.1768,
        0.1757, 0.1747, 0.1737, 0.1727, 0.1717, 0.1707, 0.1696, 0.1686, 0.1677,
        0.1667, 0.1657, 0.1647, 0.1637, 0.1627, 0.1618, 0.1608, 0.1598, 0.1589,
        0.1579, 0.1569, 0.1560, 0.1550, 0.1541, 0.1532, 0.1522, 0.1513, 0.1504,
        0.1494, 0.1485, 0.1476, 0.1467, 0.1458, 0.1449, 0.1440, 0.1431, 0.1422,
        0.1413, 0.1404, 0.1395, 0.1386, 0.1378, 0.1369, 0.1360, 0.1352, 0.1343,
        0.1334, 0.1326, 0.1317, 0.1309, 0.1301, 0.1292, 0.1284, 0.1276, 0.1267,
        0.1259, 0.1251, 0.1243, 0.1235, 0.1227, 0.1219, 0.1211, 0.1203, 0.1195,
        0.1187, 0.1179, 0.1171, 0.1163, 0.1155, 0.1148, 0.1140, 0.1132, 0.1125,
        0.1117, 0.1110, 0.1102, 0.1095, 0.1087, 0.1080, 0.1073, 0.1065, 0.1058,
        0.1051, 0.1044, 0.1036, 0.1029, 0.1022, 0.1015, 0.1008, 0.1001, 0.0994,
        0.0987, 0.0980, 0.0973, 0.0967, 0.0960, 0.0953, 0.0946, 0.0940, 0.0933,
        0.0926, 0.0920, 0.0913, 0.0907, 0.0900, 0.0894, 0.0887, 0.0881, 0.0875,
        0.0868, 0.0862, 0.0856, 0.0850, 0.0844, 0.0837, 0.0831, 0.0825, 0.0819,
        0.0813, 0.0807, 0.0801, 0.0795, 0.0789, 0.0784, 0.0778, 0.0772, 0.0766,
        0.0761, 0.0755, 0.0749, 0.0744, 0.0738, 0.0732, 0.0727, 0.0721, 0.0716,
        0.0711, 0.0705, 0.0700, 0.0694, 0.0689, 0.0684, 0.0679, 0.0673, 0.0668,
        0.0663, 0.0658, 0.0653, 0.0648, 0.0643, 0.0638, 0.0633, 0.0628, 0.0623,
        0.0618, 0.0613, 0.0608, 0.0604, 0.0599, 0.0594, 0.0589, 0.0585, 0.0580,
        0.0575, 0.0571, 0.0566, 0.0562, 0.0557, 0.0553, 0.0548, 0.0544, 0.0539,
        0.0535, 0.0531, 0.0526, 0.0522, 0.0518, 0.0514, 0.0509, 0.0505, 0.0501,
        0.0497, 0.0493, 0.0489, 0.0485, 0.0481, 0.0477, 0.0473, 0.0469, 0.0465,
        0.0461, 0.0457, 0.0453, 0.0450, 0.0446, 0.0442, 0.0438, 0.0435, 0.0431,
        0.0427, 0.0424, 0.0420, 0.0416, 0.0413, 0.0409, 0.0406, 0.0402, 0.0399,
        0.0395, 0.0392, 0.0389, 0.0385, 0.0382, 0.0379, 0.0375, 0.0372, 0.0369,
        0.0365, 0.0362, 0.0359, 0.0356, 0.0353, 0.0350, 0.0347, 0.0343, 0.0340,
        0.0337, 0.0334, 0.0331, 0.0328, 0.0325, 0.0323, 0.0320, 0.0317, 0.0314,
        0.0311, 0.0308, 0.0305, 0.0303, 0.0300, 0.0297, 0.0295, 0.0292, 0.0289,
        0.0286, 0.0284, 0.0281, 0.0279, 0.0276, 0.0274, 0.0271, 0.0268, 0.0266,
        0.0264, 0.0261, 0.0259, 0.0256, 0.0254, 0.0251, 0.0249, 0.0247, 0.0244,
        0.0242, 0.0240, 0.0237, 0.0235, 0.0233, 0.0231, 0.0229, 0.0226, 0.0224,
        0.0222, 0.0220, 0.0218, 0.0216, 0.0214, 0.0212, 0.0210, 0.0207, 0.0205,
        0.0203, 0.0201, 0.0200, 0.0198, 0.0196, 0.0194, 0.0192, 0.0190, 0.0188,
        0.0186, 0.0184, 0.0182, 0.0181, 0.0179, 0.0177, 0.0175, 0.0174, 0.0172,
        0.0170, 0.0168, 0.0167, 0.0165, 0.0163, 0.0162, 0.0160, 0.0158, 0.0157,
        0.0155, 0.0154, 0.0152, 0.0151, 0.0149, 0.0147, 0.0146, 0.0144, 0.0143,
        0.0142, 0.0140, 0.0139, 0.0137, 0.0136, 0.0134, 0.0133, 0.0132, 0.0130,
        0.0129, 0.0127, 0.0126, 0.0125, 0.0123, 0.0122, 0.0121, 0.0120, 0.0118,
        0.0117, 0.0116, 0.0115, 0.0113, 0.0112, 0.0111, 0.0110, 0.0109, 0.0107,
        0.0106, 0.0105, 0.0104, 0.0103, 0.0102, 0.0101, 0.0100, 0.0098, 0.0097,
        0.0096, 0.0095, 0.0094, 0.0093, 0.0092, 0.0091, 0.0090, 0.0089, 0.0088,
        0.0087, 0.0086, 0.0085, 0.0084, 0.0083, 0.0082, 0.0082, 0.0081, 0.0080,
        0.0079, 0.0078, 0.0077, 0.0076, 0.0075, 0.0074, 0.0074, 0.0073, 0.0072,
        0.0071, 0.0070, 0.0070, 0.0069, 0.0068, 0.0067, 0.0066, 0.0066, 0.0065,
        0.0064, 0.0063, 0.0063, 0.0062, 0.0061, 0.0061, 0.0060, 0.0059, 0.0058,
        0.0058, 0.0057, 0.0056, 0.0056, 0.0055, 0.0054, 0.0054, 0.0053, 0.0053,
        0.0052, 0.0051, 0.0051, 0.0050, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047,
        0.0047], device='cuda:0')
alpha_bar_u = tensor([0.6987, 0.6828], device='cuda:0')
start_timesteps = tensor([235, 245], device='cuda:0')
d = torch.Size([2, 4, 32, 32])
LPIPS = (tensor([[[[ 0.2887,  0.4036,  0.1904,  ...,  0.2828,  0.3137,  0.3595],
          [ 0.2365,  0.3077,  0.1498,  ...,  0.1963,  0.2153,  0.2201],
          [ 0.2055,  0.0653,  0.0283,  ...,  0.1507,  0.1948,  0.2241],
          ...,
          [ 0.7417,  0.6429,  0.4552,  ...,  0.6712,  0.6814,  0.6427],
          [ 0.6702,  0.6651,  0.5365,  ...,  0.6646,  0.6842,  0.6441],
          [ 0.5340,  0.6052,  0.5324,  ...,  0.6271,  0.6480,  0.6396]],

         [[ 0.5747,  0.6441,  0.6299,  ...,  0.7216,  0.7128,  0.6962],
          [ 0.6176,  0.6404,  0.6842,  ...,  0.7266,  0.7013,  0.6565],
          [ 0.6424,  0.7281,  0.8280,  ...,  0.7300,  0.7295,  0.7192],
          ...,
          [ 0.8451,  0.6709,  0.5209,  ...,  0.7830,  0.7670,  0.7288],
          [ 0.7723,  0.7218,  0.5637,  ...,  0.7887,  0.7928,  0.7443],
          [ 0.4870,  0.6266,  0.5714,  ...,  0.7417,  0.7500,  0.7101]],

         [[ 0.7839,  1.0342,  1.0216,  ...,  1.0154,  1.0225,  0.9206],
          [ 0.9593,  1.0203,  1.0882,  ...,  1.0502,  1.0734,  1.0275],
          [ 1.0162,  1.0868,  1.0403,  ...,  1.0315,  1.0567,  1.0539],
          ...,
          [ 0.6173,  0.4635,  0.1813,  ...,  0.8578,  0.8478,  0.8041],
          [ 0.5422,  0.4824,  0.2791,  ...,  0.8537,  0.8201,  0.7864],
          [ 0.3828,  0.4888,  0.3211,  ...,  0.8326,  0.8004,  0.6887]]],


        [[[ 0.6345,  0.6297,  0.5528,  ..., -0.2062, -0.1740, -0.1196],
          [ 0.5489,  0.5516,  0.5470,  ..., -0.2036, -0.2005, -0.1973],
          [ 0.5605,  0.5401,  0.5858,  ..., -0.1959, -0.1921, -0.1885],
          ...,
          [ 0.0791,  0.1037,  0.1121,  ...,  0.2836,  0.3162,  0.2738],
          [ 0.0678,  0.0993,  0.0799,  ...,  0.2743,  0.2514,  0.2007],
          [ 0.0466,  0.0109,  0.0591,  ...,  0.2223,  0.2266,  0.2206]],

         [[ 0.6722,  0.6878,  0.6478,  ...,  0.0892,  0.0918,  0.1249],
          [ 0.6801,  0.7201,  0.7483,  ...,  0.0893,  0.0737,  0.0670],
          [ 0.7556,  0.8107,  0.8403,  ...,  0.0886,  0.0860,  0.0941],
          ...,
          [ 0.0852,  0.0659,  0.1201,  ...,  0.3230,  0.2999,  0.2511],
          [ 0.0627,  0.0617,  0.0898,  ...,  0.3424,  0.2767,  0.2028],
          [ 0.0081, -0.0482,  0.0262,  ...,  0.2811,  0.2437,  0.1811]],

         [[ 0.7414,  0.8084,  0.7784,  ...,  0.8930,  0.8829,  0.7393],
          [ 0.8096,  0.8736,  0.8728,  ...,  0.9023,  0.9035,  0.8515],
          [ 0.8974,  0.9544,  0.9216,  ...,  0.9186,  0.9224,  0.9091],
          ...,
          [-0.0038,  0.0339,  0.1027,  ...,  0.5186,  0.4346,  0.3400],
          [-0.0232,  0.0315,  0.0723,  ...,  0.4571,  0.3566,  0.2536],
          [-0.0541, -0.0357,  0.0355,  ...,  0.3811,  0.3312,  0.1928]]]],
       device='cuda:0'), tensor([[[[1.0000, 0.5294, 0.0000,  ..., 0.5137, 0.5137, 0.5137],
          [1.0000, 0.5294, 0.0000,  ..., 0.4863, 0.4941, 0.5098],
          [1.0000, 0.5294, 0.0000,  ..., 0.3098, 0.3333, 0.3725],
          ...,
          [1.0000, 0.7922, 0.5176,  ..., 0.6471, 0.6235, 0.5922],
          [1.0000, 0.8431, 0.5922,  ..., 0.6078, 0.6235, 0.5686],
          [1.0000, 0.8275, 0.6314,  ..., 0.6392, 0.6549, 0.6353]],

         [[1.0000, 0.7176, 0.3961,  ..., 0.6902, 0.6824, 0.6784],
          [1.0000, 0.7176, 0.3961,  ..., 0.6706, 0.6784, 0.6863],
          [1.0000, 0.7176, 0.3961,  ..., 0.5373, 0.5608, 0.5882],
          ...,
          [1.0000, 0.7725, 0.4784,  ..., 0.5961, 0.5725, 0.5451],
          [1.0000, 0.8235, 0.5529,  ..., 0.5608, 0.5725, 0.5216],
          [1.0000, 0.8118, 0.5882,  ..., 0.5882, 0.6039, 0.5882]],

         [[1.0000, 0.8863, 0.7608,  ..., 0.8745, 0.8627, 0.8510],
          [1.0000, 0.8902, 0.7686,  ..., 0.8588, 0.8627, 0.8706],
          [1.0000, 0.8902, 0.7686,  ..., 0.7843, 0.8039, 0.8314],
          ...,
          [1.0000, 0.7373, 0.4039,  ..., 0.5216, 0.4902, 0.4588],
          [1.0000, 0.7882, 0.4784,  ..., 0.4824, 0.4902, 0.4353],
          [1.0000, 0.7725, 0.5137,  ..., 0.5137, 0.5255, 0.5059]]],


        [[[0.7451, 0.7843, 0.7922,  ..., 0.1255, 0.1255, 0.1255],
          [0.7451, 0.7804, 0.7843,  ..., 0.1255, 0.1255, 0.1255],
          [0.7569, 0.7804, 0.7725,  ..., 0.1255, 0.1255, 0.1255],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.0941, 0.0667, 0.0980],
          [0.0745, 0.0824, 0.0902,  ..., 0.1020, 0.0863, 0.1059],
          [0.0745, 0.0824, 0.0863,  ..., 0.0824, 0.0824, 0.0824]],

         [[0.7569, 0.7961, 0.7961,  ..., 0.2863, 0.2863, 0.2863],
          [0.7529, 0.7882, 0.7882,  ..., 0.2863, 0.2863, 0.2863],
          [0.7647, 0.7843, 0.7765,  ..., 0.2863, 0.2863, 0.2863],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.1059, 0.0824, 0.1176],
          [0.0745, 0.0824, 0.0902,  ..., 0.1098, 0.0941, 0.1176],
          [0.0745, 0.0824, 0.0863,  ..., 0.0863, 0.0863, 0.0863]],

         [[0.7294, 0.7451, 0.7176,  ..., 0.7020, 0.7020, 0.7020],
          [0.7294, 0.7412, 0.7137,  ..., 0.7020, 0.7020, 0.7020],
          [0.7412, 0.7412, 0.7059,  ..., 0.7020, 0.7020, 0.7020],
          ...,
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.0941, 0.1294],
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.1059, 0.1255],
          [0.0824, 0.0902, 0.0941,  ..., 0.0902, 0.0941, 0.0941]]]],
       device='cuda:0'))
recoverd_img = torch.Size([2, 3, 256, 256])
images are saved in outputs/output_0.png
images are saved in outputs/z_1.png
images are saved in outputs/nosample_1.png
####cond finisihed #####
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps, Selected timesteps for ddim sampler: (200,)
ldm/modules/diffusionmodules/util.py, make_ddim_sampling_parameters, Selected alphas for ddim sampler: a_t: torch.Size([200]); a_(t-1): (200,)
For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)
ddim.py alphas_cumprod = tensor([0.9991, 0.9983, 0.9974, 0.9966, 0.9957, 0.9948, 0.9940, 0.9931, 0.9922,
        0.9913, 0.9904, 0.9895, 0.9886, 0.9877, 0.9868, 0.9859, 0.9850, 0.9841,
        0.9832, 0.9822, 0.9813, 0.9804, 0.9794, 0.9785, 0.9776, 0.9766, 0.9757,
        0.9747, 0.9737, 0.9728, 0.9718, 0.9708, 0.9698, 0.9689, 0.9679, 0.9669,
        0.9659, 0.9649, 0.9639, 0.9629, 0.9619, 0.9609, 0.9599, 0.9588, 0.9578,
        0.9568, 0.9557, 0.9547, 0.9537, 0.9526, 0.9516, 0.9505, 0.9495, 0.9484,
        0.9473, 0.9463, 0.9452, 0.9441, 0.9430, 0.9420, 0.9409, 0.9398, 0.9387,
        0.9376, 0.9365, 0.9354, 0.9343, 0.9332, 0.9320, 0.9309, 0.9298, 0.9287,
        0.9275, 0.9264, 0.9252, 0.9241, 0.9229, 0.9218, 0.9206, 0.9195, 0.9183,
        0.9171, 0.9160, 0.9148, 0.9136, 0.9124, 0.9112, 0.9100, 0.9089, 0.9077,
        0.9065, 0.9052, 0.9040, 0.9028, 0.9016, 0.9004, 0.8992, 0.8979, 0.8967,
        0.8955, 0.8942, 0.8930, 0.8917, 0.8905, 0.8892, 0.8880, 0.8867, 0.8854,
        0.8842, 0.8829, 0.8816, 0.8804, 0.8791, 0.8778, 0.8765, 0.8752, 0.8739,
        0.8726, 0.8713, 0.8700, 0.8687, 0.8674, 0.8661, 0.8647, 0.8634, 0.8621,
        0.8607, 0.8594, 0.8581, 0.8567, 0.8554, 0.8540, 0.8527, 0.8513, 0.8500,
        0.8486, 0.8473, 0.8459, 0.8445, 0.8431, 0.8418, 0.8404, 0.8390, 0.8376,
        0.8362, 0.8348, 0.8334, 0.8320, 0.8306, 0.8292, 0.8278, 0.8264, 0.8250,
        0.8236, 0.8221, 0.8207, 0.8193, 0.8179, 0.8164, 0.8150, 0.8136, 0.8121,
        0.8107, 0.8092, 0.8078, 0.8063, 0.8049, 0.8034, 0.8019, 0.8005, 0.7990,
        0.7975, 0.7960, 0.7946, 0.7931, 0.7916, 0.7901, 0.7886, 0.7871, 0.7856,
        0.7842, 0.7827, 0.7812, 0.7796, 0.7781, 0.7766, 0.7751, 0.7736, 0.7721,
        0.7706, 0.7690, 0.7675, 0.7660, 0.7645, 0.7629, 0.7614, 0.7599, 0.7583,
        0.7568, 0.7552, 0.7537, 0.7521, 0.7506, 0.7490, 0.7475, 0.7459, 0.7444,
        0.7428, 0.7412, 0.7397, 0.7381, 0.7365, 0.7350, 0.7334, 0.7318, 0.7302,
        0.7286, 0.7271, 0.7255, 0.7239, 0.7223, 0.7207, 0.7191, 0.7175, 0.7159,
        0.7143, 0.7127, 0.7111, 0.7095, 0.7079, 0.7063, 0.7047, 0.7031, 0.7015,
        0.6999, 0.6982, 0.6966, 0.6950, 0.6934, 0.6918, 0.6901, 0.6885, 0.6869,
        0.6852, 0.6836, 0.6820, 0.6803, 0.6787, 0.6771, 0.6754, 0.6738, 0.6722,
        0.6705, 0.6689, 0.6672, 0.6656, 0.6639, 0.6623, 0.6606, 0.6590, 0.6573,
        0.6557, 0.6540, 0.6524, 0.6507, 0.6490, 0.6474, 0.6457, 0.6441, 0.6424,
        0.6407, 0.6391, 0.6374, 0.6357, 0.6341, 0.6324, 0.6307, 0.6291, 0.6274,
        0.6257, 0.6241, 0.6224, 0.6207, 0.6190, 0.6174, 0.6157, 0.6140, 0.6123,
        0.6107, 0.6090, 0.6073, 0.6056, 0.6039, 0.6023, 0.6006, 0.5989, 0.5972,
        0.5955, 0.5939, 0.5922, 0.5905, 0.5888, 0.5871, 0.5855, 0.5838, 0.5821,
        0.5804, 0.5787, 0.5770, 0.5754, 0.5737, 0.5720, 0.5703, 0.5686, 0.5669,
        0.5652, 0.5636, 0.5619, 0.5602, 0.5585, 0.5568, 0.5551, 0.5535, 0.5518,
        0.5501, 0.5484, 0.5467, 0.5450, 0.5434, 0.5417, 0.5400, 0.5383, 0.5366,
        0.5350, 0.5333, 0.5316, 0.5299, 0.5282, 0.5266, 0.5249, 0.5232, 0.5215,
        0.5199, 0.5182, 0.5165, 0.5148, 0.5132, 0.5115, 0.5098, 0.5082, 0.5065,
        0.5048, 0.5032, 0.5015, 0.4998, 0.4982, 0.4965, 0.4948, 0.4932, 0.4915,
        0.4898, 0.4882, 0.4865, 0.4849, 0.4832, 0.4816, 0.4799, 0.4782, 0.4766,
        0.4749, 0.4733, 0.4716, 0.4700, 0.4684, 0.4667, 0.4651, 0.4634, 0.4618,
        0.4601, 0.4585, 0.4569, 0.4552, 0.4536, 0.4520, 0.4503, 0.4487, 0.4471,
        0.4455, 0.4438, 0.4422, 0.4406, 0.4390, 0.4374, 0.4357, 0.4341, 0.4325,
        0.4309, 0.4293, 0.4277, 0.4261, 0.4245, 0.4229, 0.4213, 0.4197, 0.4181,
        0.4165, 0.4149, 0.4133, 0.4117, 0.4101, 0.4086, 0.4070, 0.4054, 0.4038,
        0.4022, 0.4007, 0.3991, 0.3975, 0.3960, 0.3944, 0.3928, 0.3913, 0.3897,
        0.3882, 0.3866, 0.3850, 0.3835, 0.3819, 0.3804, 0.3789, 0.3773, 0.3758,
        0.3742, 0.3727, 0.3712, 0.3697, 0.3681, 0.3666, 0.3651, 0.3636, 0.3621,
        0.3605, 0.3590, 0.3575, 0.3560, 0.3545, 0.3530, 0.3515, 0.3500, 0.3485,
        0.3470, 0.3456, 0.3441, 0.3426, 0.3411, 0.3396, 0.3382, 0.3367, 0.3352,
        0.3338, 0.3323, 0.3308, 0.3294, 0.3279, 0.3265, 0.3250, 0.3236, 0.3222,
        0.3207, 0.3193, 0.3178, 0.3164, 0.3150, 0.3136, 0.3122, 0.3107, 0.3093,
        0.3079, 0.3065, 0.3051, 0.3037, 0.3023, 0.3009, 0.2995, 0.2981, 0.2967,
        0.2954, 0.2940, 0.2926, 0.2912, 0.2899, 0.2885, 0.2871, 0.2858, 0.2844,
        0.2831, 0.2817, 0.2804, 0.2790, 0.2777, 0.2763, 0.2750, 0.2737, 0.2723,
        0.2710, 0.2697, 0.2684, 0.2671, 0.2658, 0.2645, 0.2631, 0.2618, 0.2606,
        0.2593, 0.2580, 0.2567, 0.2554, 0.2541, 0.2528, 0.2516, 0.2503, 0.2490,
        0.2478, 0.2465, 0.2453, 0.2440, 0.2428, 0.2415, 0.2403, 0.2391, 0.2378,
        0.2366, 0.2354, 0.2341, 0.2329, 0.2317, 0.2305, 0.2293, 0.2281, 0.2269,
        0.2257, 0.2245, 0.2233, 0.2221, 0.2209, 0.2198, 0.2186, 0.2174, 0.2163,
        0.2151, 0.2139, 0.2128, 0.2116, 0.2105, 0.2093, 0.2082, 0.2071, 0.2059,
        0.2048, 0.2037, 0.2026, 0.2014, 0.2003, 0.1992, 0.1981, 0.1970, 0.1959,
        0.1948, 0.1937, 0.1926, 0.1915, 0.1905, 0.1894, 0.1883, 0.1872, 0.1862,
        0.1851, 0.1841, 0.1830, 0.1820, 0.1809, 0.1799, 0.1788, 0.1778, 0.1768,
        0.1757, 0.1747, 0.1737, 0.1727, 0.1717, 0.1707, 0.1696, 0.1686, 0.1677,
        0.1667, 0.1657, 0.1647, 0.1637, 0.1627, 0.1618, 0.1608, 0.1598, 0.1589,
        0.1579, 0.1569, 0.1560, 0.1550, 0.1541, 0.1532, 0.1522, 0.1513, 0.1504,
        0.1494, 0.1485, 0.1476, 0.1467, 0.1458, 0.1449, 0.1440, 0.1431, 0.1422,
        0.1413, 0.1404, 0.1395, 0.1386, 0.1378, 0.1369, 0.1360, 0.1352, 0.1343,
        0.1334, 0.1326, 0.1317, 0.1309, 0.1301, 0.1292, 0.1284, 0.1276, 0.1267,
        0.1259, 0.1251, 0.1243, 0.1235, 0.1227, 0.1219, 0.1211, 0.1203, 0.1195,
        0.1187, 0.1179, 0.1171, 0.1163, 0.1155, 0.1148, 0.1140, 0.1132, 0.1125,
        0.1117, 0.1110, 0.1102, 0.1095, 0.1087, 0.1080, 0.1073, 0.1065, 0.1058,
        0.1051, 0.1044, 0.1036, 0.1029, 0.1022, 0.1015, 0.1008, 0.1001, 0.0994,
        0.0987, 0.0980, 0.0973, 0.0967, 0.0960, 0.0953, 0.0946, 0.0940, 0.0933,
        0.0926, 0.0920, 0.0913, 0.0907, 0.0900, 0.0894, 0.0887, 0.0881, 0.0875,
        0.0868, 0.0862, 0.0856, 0.0850, 0.0844, 0.0837, 0.0831, 0.0825, 0.0819,
        0.0813, 0.0807, 0.0801, 0.0795, 0.0789, 0.0784, 0.0778, 0.0772, 0.0766,
        0.0761, 0.0755, 0.0749, 0.0744, 0.0738, 0.0732, 0.0727, 0.0721, 0.0716,
        0.0711, 0.0705, 0.0700, 0.0694, 0.0689, 0.0684, 0.0679, 0.0673, 0.0668,
        0.0663, 0.0658, 0.0653, 0.0648, 0.0643, 0.0638, 0.0633, 0.0628, 0.0623,
        0.0618, 0.0613, 0.0608, 0.0604, 0.0599, 0.0594, 0.0589, 0.0585, 0.0580,
        0.0575, 0.0571, 0.0566, 0.0562, 0.0557, 0.0553, 0.0548, 0.0544, 0.0539,
        0.0535, 0.0531, 0.0526, 0.0522, 0.0518, 0.0514, 0.0509, 0.0505, 0.0501,
        0.0497, 0.0493, 0.0489, 0.0485, 0.0481, 0.0477, 0.0473, 0.0469, 0.0465,
        0.0461, 0.0457, 0.0453, 0.0450, 0.0446, 0.0442, 0.0438, 0.0435, 0.0431,
        0.0427, 0.0424, 0.0420, 0.0416, 0.0413, 0.0409, 0.0406, 0.0402, 0.0399,
        0.0395, 0.0392, 0.0389, 0.0385, 0.0382, 0.0379, 0.0375, 0.0372, 0.0369,
        0.0365, 0.0362, 0.0359, 0.0356, 0.0353, 0.0350, 0.0347, 0.0343, 0.0340,
        0.0337, 0.0334, 0.0331, 0.0328, 0.0325, 0.0323, 0.0320, 0.0317, 0.0314,
        0.0311, 0.0308, 0.0305, 0.0303, 0.0300, 0.0297, 0.0295, 0.0292, 0.0289,
        0.0286, 0.0284, 0.0281, 0.0279, 0.0276, 0.0274, 0.0271, 0.0268, 0.0266,
        0.0264, 0.0261, 0.0259, 0.0256, 0.0254, 0.0251, 0.0249, 0.0247, 0.0244,
        0.0242, 0.0240, 0.0237, 0.0235, 0.0233, 0.0231, 0.0229, 0.0226, 0.0224,
        0.0222, 0.0220, 0.0218, 0.0216, 0.0214, 0.0212, 0.0210, 0.0207, 0.0205,
        0.0203, 0.0201, 0.0200, 0.0198, 0.0196, 0.0194, 0.0192, 0.0190, 0.0188,
        0.0186, 0.0184, 0.0182, 0.0181, 0.0179, 0.0177, 0.0175, 0.0174, 0.0172,
        0.0170, 0.0168, 0.0167, 0.0165, 0.0163, 0.0162, 0.0160, 0.0158, 0.0157,
        0.0155, 0.0154, 0.0152, 0.0151, 0.0149, 0.0147, 0.0146, 0.0144, 0.0143,
        0.0142, 0.0140, 0.0139, 0.0137, 0.0136, 0.0134, 0.0133, 0.0132, 0.0130,
        0.0129, 0.0127, 0.0126, 0.0125, 0.0123, 0.0122, 0.0121, 0.0120, 0.0118,
        0.0117, 0.0116, 0.0115, 0.0113, 0.0112, 0.0111, 0.0110, 0.0109, 0.0107,
        0.0106, 0.0105, 0.0104, 0.0103, 0.0102, 0.0101, 0.0100, 0.0098, 0.0097,
        0.0096, 0.0095, 0.0094, 0.0093, 0.0092, 0.0091, 0.0090, 0.0089, 0.0088,
        0.0087, 0.0086, 0.0085, 0.0084, 0.0083, 0.0082, 0.0082, 0.0081, 0.0080,
        0.0079, 0.0078, 0.0077, 0.0076, 0.0075, 0.0074, 0.0074, 0.0073, 0.0072,
        0.0071, 0.0070, 0.0070, 0.0069, 0.0068, 0.0067, 0.0066, 0.0066, 0.0065,
        0.0064, 0.0063, 0.0063, 0.0062, 0.0061, 0.0061, 0.0060, 0.0059, 0.0058,
        0.0058, 0.0057, 0.0056, 0.0056, 0.0055, 0.0054, 0.0054, 0.0053, 0.0053,
        0.0052, 0.0051, 0.0051, 0.0050, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047,
        0.0047], device='cuda:0')
alpha_bar_u = tensor([0.7861, 0.7733], device='cuda:0')
start_timesteps = tensor([179, 188], device='cuda:0')
d = torch.Size([2, 4, 32, 32])
LPIPS = (tensor([[[[ 4.1400e-01,  4.6734e-01,  9.9380e-02,  ...,  5.7534e-01,
            5.6424e-01,  5.8464e-01],
          [ 5.0619e-01,  4.2902e-01, -6.1165e-02,  ...,  5.3812e-01,
            5.1215e-01,  5.0297e-01],
          [ 7.0889e-01,  5.1204e-01, -5.0683e-02,  ...,  5.2823e-01,
            5.2604e-01,  5.1723e-01],
          ...,
          [ 7.8402e-01,  9.3724e-01,  9.3094e-01,  ...,  9.2340e-01,
            9.6930e-01,  8.6567e-01],
          [ 7.8263e-01,  9.1944e-01,  9.2644e-01,  ...,  8.7067e-01,
            8.8363e-01,  8.1508e-01],
          [ 7.5909e-01,  8.2597e-01,  8.9420e-01,  ...,  7.0890e-01,
            6.9801e-01,  7.5753e-01]],

         [[ 5.7499e-01,  5.9196e-01,  4.0088e-01,  ...,  8.6252e-01,
            8.4081e-01,  8.2577e-01],
          [ 7.4341e-01,  7.0341e-01,  4.1383e-01,  ...,  8.7332e-01,
            8.4178e-01,  7.9644e-01],
          [ 9.4904e-01,  8.2730e-01,  5.9811e-01,  ...,  8.5543e-01,
            8.4788e-01,  8.4411e-01],
          ...,
          [ 8.1507e-01,  9.7137e-01,  9.7615e-01,  ...,  9.7802e-01,
            1.0142e+00,  9.0839e-01],
          [ 8.5225e-01,  9.4913e-01,  9.5364e-01,  ...,  9.3465e-01,
            9.2667e-01,  9.1779e-01],
          [ 7.7413e-01,  8.4409e-01,  9.1978e-01,  ...,  8.2214e-01,
            7.9111e-01,  8.1785e-01]],

         [[ 6.5900e-01,  8.0049e-01,  6.7945e-01,  ...,  1.0458e+00,
            1.0594e+00,  1.0061e+00],
          [ 8.6271e-01,  9.0762e-01,  7.5256e-01,  ...,  1.0673e+00,
            1.0939e+00,  1.0795e+00],
          [ 9.8332e-01,  9.9406e-01,  8.1367e-01,  ...,  1.0446e+00,
            1.0717e+00,  1.0897e+00],
          ...,
          [ 7.4599e-01,  9.2180e-01,  8.9957e-01,  ...,  9.5591e-01,
            9.5151e-01,  8.9338e-01],
          [ 7.6684e-01,  9.2658e-01,  9.1456e-01,  ...,  9.4468e-01,
            8.9239e-01,  8.5216e-01],
          [ 7.5192e-01,  8.7191e-01,  8.7607e-01,  ...,  8.6925e-01,
            8.1738e-01,  7.5267e-01]]],


        [[[ 5.3148e-01,  6.4999e-01,  5.8907e-01,  ..., -2.7278e-01,
           -2.1313e-01, -1.3554e-01],
          [ 5.0444e-01,  5.6506e-01,  6.4199e-01,  ..., -3.9378e-01,
           -3.3298e-01, -2.4202e-01],
          [ 5.0110e-01,  5.7250e-01,  6.6807e-01,  ..., -3.2039e-01,
           -2.4006e-01, -2.0692e-01],
          ...,
          [-3.7964e-02, -6.1960e-02, -4.7774e-02,  ...,  3.8653e-01,
            3.3715e-01,  2.7805e-01],
          [-2.7427e-02, -1.9927e-02, -3.6364e-02,  ...,  4.6857e-01,
            4.1798e-01,  4.1018e-01],
          [ 2.4456e-02,  4.4598e-02,  1.6595e-02,  ...,  3.8391e-01,
            3.9956e-01,  4.5683e-01]],

         [[ 4.9880e-01,  5.8804e-01,  5.4162e-01,  ...,  2.6628e-02,
            1.0084e-01,  1.5312e-01],
          [ 4.6830e-01,  5.3693e-01,  6.6312e-01,  ..., -7.6463e-02,
            1.7311e-03,  1.1258e-01],
          [ 4.6694e-01,  5.3908e-01,  6.7359e-01,  ...,  8.7583e-02,
            2.0551e-01,  2.5211e-01],
          ...,
          [ 2.6084e-03, -5.9614e-02, -6.1789e-02,  ...,  4.8266e-01,
            4.7424e-01,  3.6361e-01],
          [ 2.7224e-02,  9.1306e-03,  6.8977e-05,  ...,  6.0163e-01,
            5.8324e-01,  5.3983e-01],
          [-4.7772e-03,  3.5682e-02,  3.4929e-02,  ...,  4.8241e-01,
            4.7095e-01,  4.5875e-01]],

         [[ 4.1093e-01,  5.1225e-01,  5.0245e-01,  ...,  7.9624e-01,
            8.4735e-01,  5.9775e-01],
          [ 4.4409e-01,  4.8772e-01,  5.8405e-01,  ...,  7.8080e-01,
            8.6904e-01,  8.1116e-01],
          [ 4.2610e-01,  4.7423e-01,  5.6699e-01,  ...,  9.3894e-01,
            1.0415e+00,  1.0052e+00],
          ...,
          [ 1.7415e-01,  2.5873e-01,  2.7646e-01,  ...,  7.6073e-01,
            7.4916e-01,  6.1688e-01],
          [ 1.4027e-01,  2.2421e-01,  2.2736e-01,  ...,  8.2443e-01,
            7.7548e-01,  7.0370e-01],
          [ 8.2064e-02,  1.8102e-01,  1.9981e-01,  ...,  7.2381e-01,
            6.8786e-01,  5.6010e-01]]]], device='cuda:0'), tensor([[[[1.0000, 0.5294, 0.0000,  ..., 0.5137, 0.5137, 0.5137],
          [1.0000, 0.5294, 0.0000,  ..., 0.4863, 0.4941, 0.5098],
          [1.0000, 0.5294, 0.0000,  ..., 0.3098, 0.3333, 0.3725],
          ...,
          [1.0000, 0.7922, 0.5176,  ..., 0.6471, 0.6235, 0.5922],
          [1.0000, 0.8431, 0.5922,  ..., 0.6078, 0.6235, 0.5686],
          [1.0000, 0.8275, 0.6314,  ..., 0.6392, 0.6549, 0.6353]],

         [[1.0000, 0.7176, 0.3961,  ..., 0.6902, 0.6824, 0.6784],
          [1.0000, 0.7176, 0.3961,  ..., 0.6706, 0.6784, 0.6863],
          [1.0000, 0.7176, 0.3961,  ..., 0.5373, 0.5608, 0.5882],
          ...,
          [1.0000, 0.7725, 0.4784,  ..., 0.5961, 0.5725, 0.5451],
          [1.0000, 0.8235, 0.5529,  ..., 0.5608, 0.5725, 0.5216],
          [1.0000, 0.8118, 0.5882,  ..., 0.5882, 0.6039, 0.5882]],

         [[1.0000, 0.8863, 0.7608,  ..., 0.8745, 0.8627, 0.8510],
          [1.0000, 0.8902, 0.7686,  ..., 0.8588, 0.8627, 0.8706],
          [1.0000, 0.8902, 0.7686,  ..., 0.7843, 0.8039, 0.8314],
          ...,
          [1.0000, 0.7373, 0.4039,  ..., 0.5216, 0.4902, 0.4588],
          [1.0000, 0.7882, 0.4784,  ..., 0.4824, 0.4902, 0.4353],
          [1.0000, 0.7725, 0.5137,  ..., 0.5137, 0.5255, 0.5059]]],


        [[[0.7451, 0.7843, 0.7922,  ..., 0.1255, 0.1255, 0.1255],
          [0.7451, 0.7804, 0.7843,  ..., 0.1255, 0.1255, 0.1255],
          [0.7569, 0.7804, 0.7725,  ..., 0.1255, 0.1255, 0.1255],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.0941, 0.0667, 0.0980],
          [0.0745, 0.0824, 0.0902,  ..., 0.1020, 0.0863, 0.1059],
          [0.0745, 0.0824, 0.0863,  ..., 0.0824, 0.0824, 0.0824]],

         [[0.7569, 0.7961, 0.7961,  ..., 0.2863, 0.2863, 0.2863],
          [0.7529, 0.7882, 0.7882,  ..., 0.2863, 0.2863, 0.2863],
          [0.7647, 0.7843, 0.7765,  ..., 0.2863, 0.2863, 0.2863],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.1059, 0.0824, 0.1176],
          [0.0745, 0.0824, 0.0902,  ..., 0.1098, 0.0941, 0.1176],
          [0.0745, 0.0824, 0.0863,  ..., 0.0863, 0.0863, 0.0863]],

         [[0.7294, 0.7451, 0.7176,  ..., 0.7020, 0.7020, 0.7020],
          [0.7294, 0.7412, 0.7137,  ..., 0.7020, 0.7020, 0.7020],
          [0.7412, 0.7412, 0.7059,  ..., 0.7020, 0.7020, 0.7020],
          ...,
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.0941, 0.1294],
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.1059, 0.1255],
          [0.0824, 0.0902, 0.0941,  ..., 0.0902, 0.0941, 0.0941]]]],
       device='cuda:0'))
recoverd_img = torch.Size([2, 3, 256, 256])
images are saved in outputs/output_1.png
images are saved in outputs/z_2.png
images are saved in outputs/nosample_2.png
####cond finisihed #####
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps, Selected timesteps for ddim sampler: (200,)
ldm/modules/diffusionmodules/util.py, make_ddim_sampling_parameters, Selected alphas for ddim sampler: a_t: torch.Size([200]); a_(t-1): (200,)
For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)
ddim.py alphas_cumprod = tensor([0.9991, 0.9983, 0.9974, 0.9966, 0.9957, 0.9948, 0.9940, 0.9931, 0.9922,
        0.9913, 0.9904, 0.9895, 0.9886, 0.9877, 0.9868, 0.9859, 0.9850, 0.9841,
        0.9832, 0.9822, 0.9813, 0.9804, 0.9794, 0.9785, 0.9776, 0.9766, 0.9757,
        0.9747, 0.9737, 0.9728, 0.9718, 0.9708, 0.9698, 0.9689, 0.9679, 0.9669,
        0.9659, 0.9649, 0.9639, 0.9629, 0.9619, 0.9609, 0.9599, 0.9588, 0.9578,
        0.9568, 0.9557, 0.9547, 0.9537, 0.9526, 0.9516, 0.9505, 0.9495, 0.9484,
        0.9473, 0.9463, 0.9452, 0.9441, 0.9430, 0.9420, 0.9409, 0.9398, 0.9387,
        0.9376, 0.9365, 0.9354, 0.9343, 0.9332, 0.9320, 0.9309, 0.9298, 0.9287,
        0.9275, 0.9264, 0.9252, 0.9241, 0.9229, 0.9218, 0.9206, 0.9195, 0.9183,
        0.9171, 0.9160, 0.9148, 0.9136, 0.9124, 0.9112, 0.9100, 0.9089, 0.9077,
        0.9065, 0.9052, 0.9040, 0.9028, 0.9016, 0.9004, 0.8992, 0.8979, 0.8967,
        0.8955, 0.8942, 0.8930, 0.8917, 0.8905, 0.8892, 0.8880, 0.8867, 0.8854,
        0.8842, 0.8829, 0.8816, 0.8804, 0.8791, 0.8778, 0.8765, 0.8752, 0.8739,
        0.8726, 0.8713, 0.8700, 0.8687, 0.8674, 0.8661, 0.8647, 0.8634, 0.8621,
        0.8607, 0.8594, 0.8581, 0.8567, 0.8554, 0.8540, 0.8527, 0.8513, 0.8500,
        0.8486, 0.8473, 0.8459, 0.8445, 0.8431, 0.8418, 0.8404, 0.8390, 0.8376,
        0.8362, 0.8348, 0.8334, 0.8320, 0.8306, 0.8292, 0.8278, 0.8264, 0.8250,
        0.8236, 0.8221, 0.8207, 0.8193, 0.8179, 0.8164, 0.8150, 0.8136, 0.8121,
        0.8107, 0.8092, 0.8078, 0.8063, 0.8049, 0.8034, 0.8019, 0.8005, 0.7990,
        0.7975, 0.7960, 0.7946, 0.7931, 0.7916, 0.7901, 0.7886, 0.7871, 0.7856,
        0.7842, 0.7827, 0.7812, 0.7796, 0.7781, 0.7766, 0.7751, 0.7736, 0.7721,
        0.7706, 0.7690, 0.7675, 0.7660, 0.7645, 0.7629, 0.7614, 0.7599, 0.7583,
        0.7568, 0.7552, 0.7537, 0.7521, 0.7506, 0.7490, 0.7475, 0.7459, 0.7444,
        0.7428, 0.7412, 0.7397, 0.7381, 0.7365, 0.7350, 0.7334, 0.7318, 0.7302,
        0.7286, 0.7271, 0.7255, 0.7239, 0.7223, 0.7207, 0.7191, 0.7175, 0.7159,
        0.7143, 0.7127, 0.7111, 0.7095, 0.7079, 0.7063, 0.7047, 0.7031, 0.7015,
        0.6999, 0.6982, 0.6966, 0.6950, 0.6934, 0.6918, 0.6901, 0.6885, 0.6869,
        0.6852, 0.6836, 0.6820, 0.6803, 0.6787, 0.6771, 0.6754, 0.6738, 0.6722,
        0.6705, 0.6689, 0.6672, 0.6656, 0.6639, 0.6623, 0.6606, 0.6590, 0.6573,
        0.6557, 0.6540, 0.6524, 0.6507, 0.6490, 0.6474, 0.6457, 0.6441, 0.6424,
        0.6407, 0.6391, 0.6374, 0.6357, 0.6341, 0.6324, 0.6307, 0.6291, 0.6274,
        0.6257, 0.6241, 0.6224, 0.6207, 0.6190, 0.6174, 0.6157, 0.6140, 0.6123,
        0.6107, 0.6090, 0.6073, 0.6056, 0.6039, 0.6023, 0.6006, 0.5989, 0.5972,
        0.5955, 0.5939, 0.5922, 0.5905, 0.5888, 0.5871, 0.5855, 0.5838, 0.5821,
        0.5804, 0.5787, 0.5770, 0.5754, 0.5737, 0.5720, 0.5703, 0.5686, 0.5669,
        0.5652, 0.5636, 0.5619, 0.5602, 0.5585, 0.5568, 0.5551, 0.5535, 0.5518,
        0.5501, 0.5484, 0.5467, 0.5450, 0.5434, 0.5417, 0.5400, 0.5383, 0.5366,
        0.5350, 0.5333, 0.5316, 0.5299, 0.5282, 0.5266, 0.5249, 0.5232, 0.5215,
        0.5199, 0.5182, 0.5165, 0.5148, 0.5132, 0.5115, 0.5098, 0.5082, 0.5065,
        0.5048, 0.5032, 0.5015, 0.4998, 0.4982, 0.4965, 0.4948, 0.4932, 0.4915,
        0.4898, 0.4882, 0.4865, 0.4849, 0.4832, 0.4816, 0.4799, 0.4782, 0.4766,
        0.4749, 0.4733, 0.4716, 0.4700, 0.4684, 0.4667, 0.4651, 0.4634, 0.4618,
        0.4601, 0.4585, 0.4569, 0.4552, 0.4536, 0.4520, 0.4503, 0.4487, 0.4471,
        0.4455, 0.4438, 0.4422, 0.4406, 0.4390, 0.4374, 0.4357, 0.4341, 0.4325,
        0.4309, 0.4293, 0.4277, 0.4261, 0.4245, 0.4229, 0.4213, 0.4197, 0.4181,
        0.4165, 0.4149, 0.4133, 0.4117, 0.4101, 0.4086, 0.4070, 0.4054, 0.4038,
        0.4022, 0.4007, 0.3991, 0.3975, 0.3960, 0.3944, 0.3928, 0.3913, 0.3897,
        0.3882, 0.3866, 0.3850, 0.3835, 0.3819, 0.3804, 0.3789, 0.3773, 0.3758,
        0.3742, 0.3727, 0.3712, 0.3697, 0.3681, 0.3666, 0.3651, 0.3636, 0.3621,
        0.3605, 0.3590, 0.3575, 0.3560, 0.3545, 0.3530, 0.3515, 0.3500, 0.3485,
        0.3470, 0.3456, 0.3441, 0.3426, 0.3411, 0.3396, 0.3382, 0.3367, 0.3352,
        0.3338, 0.3323, 0.3308, 0.3294, 0.3279, 0.3265, 0.3250, 0.3236, 0.3222,
        0.3207, 0.3193, 0.3178, 0.3164, 0.3150, 0.3136, 0.3122, 0.3107, 0.3093,
        0.3079, 0.3065, 0.3051, 0.3037, 0.3023, 0.3009, 0.2995, 0.2981, 0.2967,
        0.2954, 0.2940, 0.2926, 0.2912, 0.2899, 0.2885, 0.2871, 0.2858, 0.2844,
        0.2831, 0.2817, 0.2804, 0.2790, 0.2777, 0.2763, 0.2750, 0.2737, 0.2723,
        0.2710, 0.2697, 0.2684, 0.2671, 0.2658, 0.2645, 0.2631, 0.2618, 0.2606,
        0.2593, 0.2580, 0.2567, 0.2554, 0.2541, 0.2528, 0.2516, 0.2503, 0.2490,
        0.2478, 0.2465, 0.2453, 0.2440, 0.2428, 0.2415, 0.2403, 0.2391, 0.2378,
        0.2366, 0.2354, 0.2341, 0.2329, 0.2317, 0.2305, 0.2293, 0.2281, 0.2269,
        0.2257, 0.2245, 0.2233, 0.2221, 0.2209, 0.2198, 0.2186, 0.2174, 0.2163,
        0.2151, 0.2139, 0.2128, 0.2116, 0.2105, 0.2093, 0.2082, 0.2071, 0.2059,
        0.2048, 0.2037, 0.2026, 0.2014, 0.2003, 0.1992, 0.1981, 0.1970, 0.1959,
        0.1948, 0.1937, 0.1926, 0.1915, 0.1905, 0.1894, 0.1883, 0.1872, 0.1862,
        0.1851, 0.1841, 0.1830, 0.1820, 0.1809, 0.1799, 0.1788, 0.1778, 0.1768,
        0.1757, 0.1747, 0.1737, 0.1727, 0.1717, 0.1707, 0.1696, 0.1686, 0.1677,
        0.1667, 0.1657, 0.1647, 0.1637, 0.1627, 0.1618, 0.1608, 0.1598, 0.1589,
        0.1579, 0.1569, 0.1560, 0.1550, 0.1541, 0.1532, 0.1522, 0.1513, 0.1504,
        0.1494, 0.1485, 0.1476, 0.1467, 0.1458, 0.1449, 0.1440, 0.1431, 0.1422,
        0.1413, 0.1404, 0.1395, 0.1386, 0.1378, 0.1369, 0.1360, 0.1352, 0.1343,
        0.1334, 0.1326, 0.1317, 0.1309, 0.1301, 0.1292, 0.1284, 0.1276, 0.1267,
        0.1259, 0.1251, 0.1243, 0.1235, 0.1227, 0.1219, 0.1211, 0.1203, 0.1195,
        0.1187, 0.1179, 0.1171, 0.1163, 0.1155, 0.1148, 0.1140, 0.1132, 0.1125,
        0.1117, 0.1110, 0.1102, 0.1095, 0.1087, 0.1080, 0.1073, 0.1065, 0.1058,
        0.1051, 0.1044, 0.1036, 0.1029, 0.1022, 0.1015, 0.1008, 0.1001, 0.0994,
        0.0987, 0.0980, 0.0973, 0.0967, 0.0960, 0.0953, 0.0946, 0.0940, 0.0933,
        0.0926, 0.0920, 0.0913, 0.0907, 0.0900, 0.0894, 0.0887, 0.0881, 0.0875,
        0.0868, 0.0862, 0.0856, 0.0850, 0.0844, 0.0837, 0.0831, 0.0825, 0.0819,
        0.0813, 0.0807, 0.0801, 0.0795, 0.0789, 0.0784, 0.0778, 0.0772, 0.0766,
        0.0761, 0.0755, 0.0749, 0.0744, 0.0738, 0.0732, 0.0727, 0.0721, 0.0716,
        0.0711, 0.0705, 0.0700, 0.0694, 0.0689, 0.0684, 0.0679, 0.0673, 0.0668,
        0.0663, 0.0658, 0.0653, 0.0648, 0.0643, 0.0638, 0.0633, 0.0628, 0.0623,
        0.0618, 0.0613, 0.0608, 0.0604, 0.0599, 0.0594, 0.0589, 0.0585, 0.0580,
        0.0575, 0.0571, 0.0566, 0.0562, 0.0557, 0.0553, 0.0548, 0.0544, 0.0539,
        0.0535, 0.0531, 0.0526, 0.0522, 0.0518, 0.0514, 0.0509, 0.0505, 0.0501,
        0.0497, 0.0493, 0.0489, 0.0485, 0.0481, 0.0477, 0.0473, 0.0469, 0.0465,
        0.0461, 0.0457, 0.0453, 0.0450, 0.0446, 0.0442, 0.0438, 0.0435, 0.0431,
        0.0427, 0.0424, 0.0420, 0.0416, 0.0413, 0.0409, 0.0406, 0.0402, 0.0399,
        0.0395, 0.0392, 0.0389, 0.0385, 0.0382, 0.0379, 0.0375, 0.0372, 0.0369,
        0.0365, 0.0362, 0.0359, 0.0356, 0.0353, 0.0350, 0.0347, 0.0343, 0.0340,
        0.0337, 0.0334, 0.0331, 0.0328, 0.0325, 0.0323, 0.0320, 0.0317, 0.0314,
        0.0311, 0.0308, 0.0305, 0.0303, 0.0300, 0.0297, 0.0295, 0.0292, 0.0289,
        0.0286, 0.0284, 0.0281, 0.0279, 0.0276, 0.0274, 0.0271, 0.0268, 0.0266,
        0.0264, 0.0261, 0.0259, 0.0256, 0.0254, 0.0251, 0.0249, 0.0247, 0.0244,
        0.0242, 0.0240, 0.0237, 0.0235, 0.0233, 0.0231, 0.0229, 0.0226, 0.0224,
        0.0222, 0.0220, 0.0218, 0.0216, 0.0214, 0.0212, 0.0210, 0.0207, 0.0205,
        0.0203, 0.0201, 0.0200, 0.0198, 0.0196, 0.0194, 0.0192, 0.0190, 0.0188,
        0.0186, 0.0184, 0.0182, 0.0181, 0.0179, 0.0177, 0.0175, 0.0174, 0.0172,
        0.0170, 0.0168, 0.0167, 0.0165, 0.0163, 0.0162, 0.0160, 0.0158, 0.0157,
        0.0155, 0.0154, 0.0152, 0.0151, 0.0149, 0.0147, 0.0146, 0.0144, 0.0143,
        0.0142, 0.0140, 0.0139, 0.0137, 0.0136, 0.0134, 0.0133, 0.0132, 0.0130,
        0.0129, 0.0127, 0.0126, 0.0125, 0.0123, 0.0122, 0.0121, 0.0120, 0.0118,
        0.0117, 0.0116, 0.0115, 0.0113, 0.0112, 0.0111, 0.0110, 0.0109, 0.0107,
        0.0106, 0.0105, 0.0104, 0.0103, 0.0102, 0.0101, 0.0100, 0.0098, 0.0097,
        0.0096, 0.0095, 0.0094, 0.0093, 0.0092, 0.0091, 0.0090, 0.0089, 0.0088,
        0.0087, 0.0086, 0.0085, 0.0084, 0.0083, 0.0082, 0.0082, 0.0081, 0.0080,
        0.0079, 0.0078, 0.0077, 0.0076, 0.0075, 0.0074, 0.0074, 0.0073, 0.0072,
        0.0071, 0.0070, 0.0070, 0.0069, 0.0068, 0.0067, 0.0066, 0.0066, 0.0065,
        0.0064, 0.0063, 0.0063, 0.0062, 0.0061, 0.0061, 0.0060, 0.0059, 0.0058,
        0.0058, 0.0057, 0.0056, 0.0056, 0.0055, 0.0054, 0.0054, 0.0053, 0.0053,
        0.0052, 0.0051, 0.0051, 0.0050, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047,
        0.0047], device='cuda:0')
alpha_bar_u = tensor([0.8535, 0.8439], device='cuda:0')
start_timesteps = tensor([132, 139], device='cuda:0')
d = torch.Size([2, 4, 32, 32])
LPIPS = (tensor([[[[0.6627, 0.7494, 0.6684,  ..., 0.7714, 0.7585, 0.7254],
          [0.6957, 0.7356, 0.6312,  ..., 0.7557, 0.7365, 0.6816],
          [0.7465, 0.7271, 0.5890,  ..., 0.7069, 0.7116, 0.7172],
          ...,
          [1.0365, 0.9752, 0.8360,  ..., 0.8273, 0.8180, 0.7740],
          [0.9949, 0.9913, 0.8780,  ..., 0.8529, 0.8573, 0.8181],
          [0.9662, 0.9983, 0.8976,  ..., 0.7969, 0.8266, 0.8023]],

         [[0.6913, 0.7566, 0.7069,  ..., 0.8923, 0.8987, 0.8710],
          [0.7604, 0.7890, 0.7177,  ..., 0.8926, 0.8746, 0.8336],
          [0.8006, 0.7887, 0.6958,  ..., 0.8677, 0.8623, 0.9158],
          ...,
          [1.0562, 1.0009, 0.8509,  ..., 0.8698, 0.8471, 0.8143],
          [1.0554, 1.0199, 0.8947,  ..., 0.9055, 0.9116, 0.9027],
          [0.9527, 1.0102, 0.9483,  ..., 0.8657, 0.9146, 0.8584]],

         [[0.7329, 0.8503, 0.8544,  ..., 0.9757, 0.9998, 0.9374],
          [0.8351, 0.8907, 0.8653,  ..., 0.9871, 1.0109, 0.9990],
          [0.8895, 0.9112, 0.8436,  ..., 0.9884, 1.0058, 1.0270],
          ...,
          [1.0055, 0.9818, 0.7463,  ..., 0.9261, 0.9293, 0.8964],
          [0.9964, 0.9844, 0.8317,  ..., 0.9450, 0.9278, 0.9199],
          [0.9120, 1.0018, 0.8759,  ..., 0.9544, 0.9462, 0.8346]]],


        [[[0.6573, 0.6992, 0.7072,  ..., 0.1479, 0.1743, 0.1973],
          [0.6777, 0.6852, 0.6758,  ..., 0.1167, 0.1389, 0.1168],
          [0.6584, 0.6567, 0.6326,  ..., 0.1119, 0.1416, 0.1339],
          ...,
          [0.4266, 0.5571, 0.5290,  ..., 0.1764, 0.1329, 0.0639],
          [0.3683, 0.4902, 0.4444,  ..., 0.1221, 0.1056, 0.0850],
          [0.3035, 0.3734, 0.3785,  ..., 0.0065, 0.0015, 0.1288]],

         [[0.7140, 0.7549, 0.8045,  ..., 0.3876, 0.4091, 0.4279],
          [0.7623, 0.7593, 0.8062,  ..., 0.3593, 0.3632, 0.3544],
          [0.7602, 0.7391, 0.7799,  ..., 0.3293, 0.3617, 0.3946],
          ...,
          [0.3276, 0.3934, 0.2869,  ..., 0.3585, 0.2518, 0.1229],
          [0.2703, 0.3104, 0.2225,  ..., 0.2880, 0.2700, 0.1673],
          [0.1473, 0.1613, 0.1585,  ..., 0.1320, 0.0844, 0.1288]],

         [[0.7047, 0.7885, 0.8470,  ..., 0.9280, 0.9629, 0.8123],
          [0.7813, 0.8001, 0.8231,  ..., 0.9530, 0.9733, 0.9388],
          [0.7869, 0.7894, 0.8011,  ..., 0.9249, 0.9452, 0.9639],
          ...,
          [0.2409, 0.4052, 0.3370,  ..., 0.6208, 0.4694, 0.2912],
          [0.1921, 0.3241, 0.2703,  ..., 0.4867, 0.4408, 0.2837],
          [0.1114, 0.1754, 0.1880,  ..., 0.3205, 0.2720, 0.1945]]]],
       device='cuda:0'), tensor([[[[1.0000, 0.5294, 0.0000,  ..., 0.5137, 0.5137, 0.5137],
          [1.0000, 0.5294, 0.0000,  ..., 0.4863, 0.4941, 0.5098],
          [1.0000, 0.5294, 0.0000,  ..., 0.3098, 0.3333, 0.3725],
          ...,
          [1.0000, 0.7922, 0.5176,  ..., 0.6471, 0.6235, 0.5922],
          [1.0000, 0.8431, 0.5922,  ..., 0.6078, 0.6235, 0.5686],
          [1.0000, 0.8275, 0.6314,  ..., 0.6392, 0.6549, 0.6353]],

         [[1.0000, 0.7176, 0.3961,  ..., 0.6902, 0.6824, 0.6784],
          [1.0000, 0.7176, 0.3961,  ..., 0.6706, 0.6784, 0.6863],
          [1.0000, 0.7176, 0.3961,  ..., 0.5373, 0.5608, 0.5882],
          ...,
          [1.0000, 0.7725, 0.4784,  ..., 0.5961, 0.5725, 0.5451],
          [1.0000, 0.8235, 0.5529,  ..., 0.5608, 0.5725, 0.5216],
          [1.0000, 0.8118, 0.5882,  ..., 0.5882, 0.6039, 0.5882]],

         [[1.0000, 0.8863, 0.7608,  ..., 0.8745, 0.8627, 0.8510],
          [1.0000, 0.8902, 0.7686,  ..., 0.8588, 0.8627, 0.8706],
          [1.0000, 0.8902, 0.7686,  ..., 0.7843, 0.8039, 0.8314],
          ...,
          [1.0000, 0.7373, 0.4039,  ..., 0.5216, 0.4902, 0.4588],
          [1.0000, 0.7882, 0.4784,  ..., 0.4824, 0.4902, 0.4353],
          [1.0000, 0.7725, 0.5137,  ..., 0.5137, 0.5255, 0.5059]]],


        [[[0.7451, 0.7843, 0.7922,  ..., 0.1255, 0.1255, 0.1255],
          [0.7451, 0.7804, 0.7843,  ..., 0.1255, 0.1255, 0.1255],
          [0.7569, 0.7804, 0.7725,  ..., 0.1255, 0.1255, 0.1255],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.0941, 0.0667, 0.0980],
          [0.0745, 0.0824, 0.0902,  ..., 0.1020, 0.0863, 0.1059],
          [0.0745, 0.0824, 0.0863,  ..., 0.0824, 0.0824, 0.0824]],

         [[0.7569, 0.7961, 0.7961,  ..., 0.2863, 0.2863, 0.2863],
          [0.7529, 0.7882, 0.7882,  ..., 0.2863, 0.2863, 0.2863],
          [0.7647, 0.7843, 0.7765,  ..., 0.2863, 0.2863, 0.2863],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.1059, 0.0824, 0.1176],
          [0.0745, 0.0824, 0.0902,  ..., 0.1098, 0.0941, 0.1176],
          [0.0745, 0.0824, 0.0863,  ..., 0.0863, 0.0863, 0.0863]],

         [[0.7294, 0.7451, 0.7176,  ..., 0.7020, 0.7020, 0.7020],
          [0.7294, 0.7412, 0.7137,  ..., 0.7020, 0.7020, 0.7020],
          [0.7412, 0.7412, 0.7059,  ..., 0.7020, 0.7020, 0.7020],
          ...,
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.0941, 0.1294],
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.1059, 0.1255],
          [0.0824, 0.0902, 0.0941,  ..., 0.0902, 0.0941, 0.0941]]]],
       device='cuda:0'))
recoverd_img = torch.Size([2, 3, 256, 256])
images are saved in outputs/output_2.png
images are saved in outputs/z_3.png
images are saved in outputs/nosample_3.png
####cond finisihed #####
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps, Selected timesteps for ddim sampler: (200,)
ldm/modules/diffusionmodules/util.py, make_ddim_sampling_parameters, Selected alphas for ddim sampler: a_t: torch.Size([200]); a_(t-1): (200,)
For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)
ddim.py alphas_cumprod = tensor([0.9991, 0.9983, 0.9974, 0.9966, 0.9957, 0.9948, 0.9940, 0.9931, 0.9922,
        0.9913, 0.9904, 0.9895, 0.9886, 0.9877, 0.9868, 0.9859, 0.9850, 0.9841,
        0.9832, 0.9822, 0.9813, 0.9804, 0.9794, 0.9785, 0.9776, 0.9766, 0.9757,
        0.9747, 0.9737, 0.9728, 0.9718, 0.9708, 0.9698, 0.9689, 0.9679, 0.9669,
        0.9659, 0.9649, 0.9639, 0.9629, 0.9619, 0.9609, 0.9599, 0.9588, 0.9578,
        0.9568, 0.9557, 0.9547, 0.9537, 0.9526, 0.9516, 0.9505, 0.9495, 0.9484,
        0.9473, 0.9463, 0.9452, 0.9441, 0.9430, 0.9420, 0.9409, 0.9398, 0.9387,
        0.9376, 0.9365, 0.9354, 0.9343, 0.9332, 0.9320, 0.9309, 0.9298, 0.9287,
        0.9275, 0.9264, 0.9252, 0.9241, 0.9229, 0.9218, 0.9206, 0.9195, 0.9183,
        0.9171, 0.9160, 0.9148, 0.9136, 0.9124, 0.9112, 0.9100, 0.9089, 0.9077,
        0.9065, 0.9052, 0.9040, 0.9028, 0.9016, 0.9004, 0.8992, 0.8979, 0.8967,
        0.8955, 0.8942, 0.8930, 0.8917, 0.8905, 0.8892, 0.8880, 0.8867, 0.8854,
        0.8842, 0.8829, 0.8816, 0.8804, 0.8791, 0.8778, 0.8765, 0.8752, 0.8739,
        0.8726, 0.8713, 0.8700, 0.8687, 0.8674, 0.8661, 0.8647, 0.8634, 0.8621,
        0.8607, 0.8594, 0.8581, 0.8567, 0.8554, 0.8540, 0.8527, 0.8513, 0.8500,
        0.8486, 0.8473, 0.8459, 0.8445, 0.8431, 0.8418, 0.8404, 0.8390, 0.8376,
        0.8362, 0.8348, 0.8334, 0.8320, 0.8306, 0.8292, 0.8278, 0.8264, 0.8250,
        0.8236, 0.8221, 0.8207, 0.8193, 0.8179, 0.8164, 0.8150, 0.8136, 0.8121,
        0.8107, 0.8092, 0.8078, 0.8063, 0.8049, 0.8034, 0.8019, 0.8005, 0.7990,
        0.7975, 0.7960, 0.7946, 0.7931, 0.7916, 0.7901, 0.7886, 0.7871, 0.7856,
        0.7842, 0.7827, 0.7812, 0.7796, 0.7781, 0.7766, 0.7751, 0.7736, 0.7721,
        0.7706, 0.7690, 0.7675, 0.7660, 0.7645, 0.7629, 0.7614, 0.7599, 0.7583,
        0.7568, 0.7552, 0.7537, 0.7521, 0.7506, 0.7490, 0.7475, 0.7459, 0.7444,
        0.7428, 0.7412, 0.7397, 0.7381, 0.7365, 0.7350, 0.7334, 0.7318, 0.7302,
        0.7286, 0.7271, 0.7255, 0.7239, 0.7223, 0.7207, 0.7191, 0.7175, 0.7159,
        0.7143, 0.7127, 0.7111, 0.7095, 0.7079, 0.7063, 0.7047, 0.7031, 0.7015,
        0.6999, 0.6982, 0.6966, 0.6950, 0.6934, 0.6918, 0.6901, 0.6885, 0.6869,
        0.6852, 0.6836, 0.6820, 0.6803, 0.6787, 0.6771, 0.6754, 0.6738, 0.6722,
        0.6705, 0.6689, 0.6672, 0.6656, 0.6639, 0.6623, 0.6606, 0.6590, 0.6573,
        0.6557, 0.6540, 0.6524, 0.6507, 0.6490, 0.6474, 0.6457, 0.6441, 0.6424,
        0.6407, 0.6391, 0.6374, 0.6357, 0.6341, 0.6324, 0.6307, 0.6291, 0.6274,
        0.6257, 0.6241, 0.6224, 0.6207, 0.6190, 0.6174, 0.6157, 0.6140, 0.6123,
        0.6107, 0.6090, 0.6073, 0.6056, 0.6039, 0.6023, 0.6006, 0.5989, 0.5972,
        0.5955, 0.5939, 0.5922, 0.5905, 0.5888, 0.5871, 0.5855, 0.5838, 0.5821,
        0.5804, 0.5787, 0.5770, 0.5754, 0.5737, 0.5720, 0.5703, 0.5686, 0.5669,
        0.5652, 0.5636, 0.5619, 0.5602, 0.5585, 0.5568, 0.5551, 0.5535, 0.5518,
        0.5501, 0.5484, 0.5467, 0.5450, 0.5434, 0.5417, 0.5400, 0.5383, 0.5366,
        0.5350, 0.5333, 0.5316, 0.5299, 0.5282, 0.5266, 0.5249, 0.5232, 0.5215,
        0.5199, 0.5182, 0.5165, 0.5148, 0.5132, 0.5115, 0.5098, 0.5082, 0.5065,
        0.5048, 0.5032, 0.5015, 0.4998, 0.4982, 0.4965, 0.4948, 0.4932, 0.4915,
        0.4898, 0.4882, 0.4865, 0.4849, 0.4832, 0.4816, 0.4799, 0.4782, 0.4766,
        0.4749, 0.4733, 0.4716, 0.4700, 0.4684, 0.4667, 0.4651, 0.4634, 0.4618,
        0.4601, 0.4585, 0.4569, 0.4552, 0.4536, 0.4520, 0.4503, 0.4487, 0.4471,
        0.4455, 0.4438, 0.4422, 0.4406, 0.4390, 0.4374, 0.4357, 0.4341, 0.4325,
        0.4309, 0.4293, 0.4277, 0.4261, 0.4245, 0.4229, 0.4213, 0.4197, 0.4181,
        0.4165, 0.4149, 0.4133, 0.4117, 0.4101, 0.4086, 0.4070, 0.4054, 0.4038,
        0.4022, 0.4007, 0.3991, 0.3975, 0.3960, 0.3944, 0.3928, 0.3913, 0.3897,
        0.3882, 0.3866, 0.3850, 0.3835, 0.3819, 0.3804, 0.3789, 0.3773, 0.3758,
        0.3742, 0.3727, 0.3712, 0.3697, 0.3681, 0.3666, 0.3651, 0.3636, 0.3621,
        0.3605, 0.3590, 0.3575, 0.3560, 0.3545, 0.3530, 0.3515, 0.3500, 0.3485,
        0.3470, 0.3456, 0.3441, 0.3426, 0.3411, 0.3396, 0.3382, 0.3367, 0.3352,
        0.3338, 0.3323, 0.3308, 0.3294, 0.3279, 0.3265, 0.3250, 0.3236, 0.3222,
        0.3207, 0.3193, 0.3178, 0.3164, 0.3150, 0.3136, 0.3122, 0.3107, 0.3093,
        0.3079, 0.3065, 0.3051, 0.3037, 0.3023, 0.3009, 0.2995, 0.2981, 0.2967,
        0.2954, 0.2940, 0.2926, 0.2912, 0.2899, 0.2885, 0.2871, 0.2858, 0.2844,
        0.2831, 0.2817, 0.2804, 0.2790, 0.2777, 0.2763, 0.2750, 0.2737, 0.2723,
        0.2710, 0.2697, 0.2684, 0.2671, 0.2658, 0.2645, 0.2631, 0.2618, 0.2606,
        0.2593, 0.2580, 0.2567, 0.2554, 0.2541, 0.2528, 0.2516, 0.2503, 0.2490,
        0.2478, 0.2465, 0.2453, 0.2440, 0.2428, 0.2415, 0.2403, 0.2391, 0.2378,
        0.2366, 0.2354, 0.2341, 0.2329, 0.2317, 0.2305, 0.2293, 0.2281, 0.2269,
        0.2257, 0.2245, 0.2233, 0.2221, 0.2209, 0.2198, 0.2186, 0.2174, 0.2163,
        0.2151, 0.2139, 0.2128, 0.2116, 0.2105, 0.2093, 0.2082, 0.2071, 0.2059,
        0.2048, 0.2037, 0.2026, 0.2014, 0.2003, 0.1992, 0.1981, 0.1970, 0.1959,
        0.1948, 0.1937, 0.1926, 0.1915, 0.1905, 0.1894, 0.1883, 0.1872, 0.1862,
        0.1851, 0.1841, 0.1830, 0.1820, 0.1809, 0.1799, 0.1788, 0.1778, 0.1768,
        0.1757, 0.1747, 0.1737, 0.1727, 0.1717, 0.1707, 0.1696, 0.1686, 0.1677,
        0.1667, 0.1657, 0.1647, 0.1637, 0.1627, 0.1618, 0.1608, 0.1598, 0.1589,
        0.1579, 0.1569, 0.1560, 0.1550, 0.1541, 0.1532, 0.1522, 0.1513, 0.1504,
        0.1494, 0.1485, 0.1476, 0.1467, 0.1458, 0.1449, 0.1440, 0.1431, 0.1422,
        0.1413, 0.1404, 0.1395, 0.1386, 0.1378, 0.1369, 0.1360, 0.1352, 0.1343,
        0.1334, 0.1326, 0.1317, 0.1309, 0.1301, 0.1292, 0.1284, 0.1276, 0.1267,
        0.1259, 0.1251, 0.1243, 0.1235, 0.1227, 0.1219, 0.1211, 0.1203, 0.1195,
        0.1187, 0.1179, 0.1171, 0.1163, 0.1155, 0.1148, 0.1140, 0.1132, 0.1125,
        0.1117, 0.1110, 0.1102, 0.1095, 0.1087, 0.1080, 0.1073, 0.1065, 0.1058,
        0.1051, 0.1044, 0.1036, 0.1029, 0.1022, 0.1015, 0.1008, 0.1001, 0.0994,
        0.0987, 0.0980, 0.0973, 0.0967, 0.0960, 0.0953, 0.0946, 0.0940, 0.0933,
        0.0926, 0.0920, 0.0913, 0.0907, 0.0900, 0.0894, 0.0887, 0.0881, 0.0875,
        0.0868, 0.0862, 0.0856, 0.0850, 0.0844, 0.0837, 0.0831, 0.0825, 0.0819,
        0.0813, 0.0807, 0.0801, 0.0795, 0.0789, 0.0784, 0.0778, 0.0772, 0.0766,
        0.0761, 0.0755, 0.0749, 0.0744, 0.0738, 0.0732, 0.0727, 0.0721, 0.0716,
        0.0711, 0.0705, 0.0700, 0.0694, 0.0689, 0.0684, 0.0679, 0.0673, 0.0668,
        0.0663, 0.0658, 0.0653, 0.0648, 0.0643, 0.0638, 0.0633, 0.0628, 0.0623,
        0.0618, 0.0613, 0.0608, 0.0604, 0.0599, 0.0594, 0.0589, 0.0585, 0.0580,
        0.0575, 0.0571, 0.0566, 0.0562, 0.0557, 0.0553, 0.0548, 0.0544, 0.0539,
        0.0535, 0.0531, 0.0526, 0.0522, 0.0518, 0.0514, 0.0509, 0.0505, 0.0501,
        0.0497, 0.0493, 0.0489, 0.0485, 0.0481, 0.0477, 0.0473, 0.0469, 0.0465,
        0.0461, 0.0457, 0.0453, 0.0450, 0.0446, 0.0442, 0.0438, 0.0435, 0.0431,
        0.0427, 0.0424, 0.0420, 0.0416, 0.0413, 0.0409, 0.0406, 0.0402, 0.0399,
        0.0395, 0.0392, 0.0389, 0.0385, 0.0382, 0.0379, 0.0375, 0.0372, 0.0369,
        0.0365, 0.0362, 0.0359, 0.0356, 0.0353, 0.0350, 0.0347, 0.0343, 0.0340,
        0.0337, 0.0334, 0.0331, 0.0328, 0.0325, 0.0323, 0.0320, 0.0317, 0.0314,
        0.0311, 0.0308, 0.0305, 0.0303, 0.0300, 0.0297, 0.0295, 0.0292, 0.0289,
        0.0286, 0.0284, 0.0281, 0.0279, 0.0276, 0.0274, 0.0271, 0.0268, 0.0266,
        0.0264, 0.0261, 0.0259, 0.0256, 0.0254, 0.0251, 0.0249, 0.0247, 0.0244,
        0.0242, 0.0240, 0.0237, 0.0235, 0.0233, 0.0231, 0.0229, 0.0226, 0.0224,
        0.0222, 0.0220, 0.0218, 0.0216, 0.0214, 0.0212, 0.0210, 0.0207, 0.0205,
        0.0203, 0.0201, 0.0200, 0.0198, 0.0196, 0.0194, 0.0192, 0.0190, 0.0188,
        0.0186, 0.0184, 0.0182, 0.0181, 0.0179, 0.0177, 0.0175, 0.0174, 0.0172,
        0.0170, 0.0168, 0.0167, 0.0165, 0.0163, 0.0162, 0.0160, 0.0158, 0.0157,
        0.0155, 0.0154, 0.0152, 0.0151, 0.0149, 0.0147, 0.0146, 0.0144, 0.0143,
        0.0142, 0.0140, 0.0139, 0.0137, 0.0136, 0.0134, 0.0133, 0.0132, 0.0130,
        0.0129, 0.0127, 0.0126, 0.0125, 0.0123, 0.0122, 0.0121, 0.0120, 0.0118,
        0.0117, 0.0116, 0.0115, 0.0113, 0.0112, 0.0111, 0.0110, 0.0109, 0.0107,
        0.0106, 0.0105, 0.0104, 0.0103, 0.0102, 0.0101, 0.0100, 0.0098, 0.0097,
        0.0096, 0.0095, 0.0094, 0.0093, 0.0092, 0.0091, 0.0090, 0.0089, 0.0088,
        0.0087, 0.0086, 0.0085, 0.0084, 0.0083, 0.0082, 0.0082, 0.0081, 0.0080,
        0.0079, 0.0078, 0.0077, 0.0076, 0.0075, 0.0074, 0.0074, 0.0073, 0.0072,
        0.0071, 0.0070, 0.0070, 0.0069, 0.0068, 0.0067, 0.0066, 0.0066, 0.0065,
        0.0064, 0.0063, 0.0063, 0.0062, 0.0061, 0.0061, 0.0060, 0.0059, 0.0058,
        0.0058, 0.0057, 0.0056, 0.0056, 0.0055, 0.0054, 0.0054, 0.0053, 0.0053,
        0.0052, 0.0051, 0.0051, 0.0050, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047,
        0.0047], device='cuda:0')
alpha_bar_u = tensor([0.9023, 0.8955], device='cuda:0')
start_timesteps = tensor([94, 99], device='cuda:0')
d = torch.Size([2, 4, 32, 32])
LPIPS = (tensor([[[[ 0.2224,  0.3949,  0.0823,  ..., -0.1021, -0.0780, -0.0640],
          [ 0.2427,  0.2085, -0.0964,  ..., -0.2159, -0.1805, -0.1272],
          [ 0.3209,  0.1317, -0.1472,  ..., -0.2860, -0.2196, -0.1693],
          ...,
          [ 1.0119,  0.9812,  0.5991,  ...,  0.6583,  0.7859,  0.7963],
          [ 0.8869,  0.9025,  0.5380,  ...,  0.7232,  0.7924,  0.7362],
          [ 0.5054,  0.6807,  0.6094,  ...,  0.5597,  0.5625,  0.6433]],

         [[ 0.4143,  0.5607,  0.3982,  ...,  0.2854,  0.2641,  0.2104],
          [ 0.4848,  0.5012,  0.3296,  ...,  0.1891,  0.1737,  0.1848],
          [ 0.5974,  0.5515,  0.3974,  ...,  0.1615,  0.1779,  0.2260],
          ...,
          [ 1.0088,  0.7759,  0.3514,  ...,  0.6883,  0.8091,  0.8318],
          [ 0.8578,  0.8237,  0.3207,  ...,  0.7431,  0.8259,  0.8168],
          [ 0.3524,  0.6036,  0.5285,  ...,  0.6337,  0.5945,  0.6691]],

         [[ 0.4692,  0.7963,  0.6501,  ...,  0.7120,  0.6914,  0.4731],
          [ 0.6430,  0.7973,  0.6213,  ...,  0.7107,  0.7295,  0.6464],
          [ 0.7955,  0.8347,  0.6526,  ...,  0.7236,  0.7426,  0.7055],
          ...,
          [ 0.7282,  0.6487, -0.0137,  ...,  0.6997,  0.7873,  0.7907],
          [ 0.6365,  0.6628,  0.0097,  ...,  0.7422,  0.7490,  0.7005],
          [ 0.2840,  0.5748,  0.3083,  ...,  0.6100,  0.5733,  0.5715]]],


        [[[ 0.6001,  0.6909,  0.7226,  ...,  0.2694,  0.2648,  0.2899],
          [ 0.5439,  0.6244,  0.7460,  ...,  0.2567,  0.2681,  0.2345],
          [ 0.5422,  0.6320,  0.7338,  ...,  0.2580,  0.2840,  0.2672],
          ...,
          [-0.2526, -0.2156, -0.1166,  ...,  0.0190,  0.0316,  0.0803],
          [-0.2609, -0.2311, -0.1263,  ..., -0.0780, -0.1220, -0.0522],
          [-0.2356, -0.3365, -0.1371,  ...,  0.0146,  0.0602,  0.0985]],

         [[ 0.6112,  0.7053,  0.7614,  ...,  0.4507,  0.4635,  0.4842],
          [ 0.6002,  0.7059,  0.8511,  ...,  0.4434,  0.4587,  0.4504],
          [ 0.6049,  0.7263,  0.8448,  ...,  0.4468,  0.4766,  0.4912],
          ...,
          [-0.1933, -0.2045, -0.0274,  ...,  0.1705,  0.2175,  0.2589],
          [-0.2144, -0.2292, -0.0501,  ...,  0.0467,  0.0197,  0.1096],
          [-0.2315, -0.3152, -0.0648,  ...,  0.1365,  0.1635,  0.1520]],

         [[ 0.6074,  0.7640,  0.8330,  ...,  0.8933,  0.9191,  0.8163],
          [ 0.6545,  0.7699,  0.8741,  ...,  0.9303,  0.9609,  0.9363],
          [ 0.6820,  0.7950,  0.8752,  ...,  0.9372,  0.9596,  0.9667],
          ...,
          [-0.3001, -0.2257, -0.0299,  ...,  0.5645,  0.6328,  0.5802],
          [-0.3298, -0.2844, -0.0870,  ...,  0.3601,  0.3552,  0.3662],
          [-0.3064, -0.3491, -0.1606,  ...,  0.3506,  0.3797,  0.2710]]]],
       device='cuda:0'), tensor([[[[1.0000, 0.5294, 0.0000,  ..., 0.5137, 0.5137, 0.5137],
          [1.0000, 0.5294, 0.0000,  ..., 0.4863, 0.4941, 0.5098],
          [1.0000, 0.5294, 0.0000,  ..., 0.3098, 0.3333, 0.3725],
          ...,
          [1.0000, 0.7922, 0.5176,  ..., 0.6471, 0.6235, 0.5922],
          [1.0000, 0.8431, 0.5922,  ..., 0.6078, 0.6235, 0.5686],
          [1.0000, 0.8275, 0.6314,  ..., 0.6392, 0.6549, 0.6353]],

         [[1.0000, 0.7176, 0.3961,  ..., 0.6902, 0.6824, 0.6784],
          [1.0000, 0.7176, 0.3961,  ..., 0.6706, 0.6784, 0.6863],
          [1.0000, 0.7176, 0.3961,  ..., 0.5373, 0.5608, 0.5882],
          ...,
          [1.0000, 0.7725, 0.4784,  ..., 0.5961, 0.5725, 0.5451],
          [1.0000, 0.8235, 0.5529,  ..., 0.5608, 0.5725, 0.5216],
          [1.0000, 0.8118, 0.5882,  ..., 0.5882, 0.6039, 0.5882]],

         [[1.0000, 0.8863, 0.7608,  ..., 0.8745, 0.8627, 0.8510],
          [1.0000, 0.8902, 0.7686,  ..., 0.8588, 0.8627, 0.8706],
          [1.0000, 0.8902, 0.7686,  ..., 0.7843, 0.8039, 0.8314],
          ...,
          [1.0000, 0.7373, 0.4039,  ..., 0.5216, 0.4902, 0.4588],
          [1.0000, 0.7882, 0.4784,  ..., 0.4824, 0.4902, 0.4353],
          [1.0000, 0.7725, 0.5137,  ..., 0.5137, 0.5255, 0.5059]]],


        [[[0.7451, 0.7843, 0.7922,  ..., 0.1255, 0.1255, 0.1255],
          [0.7451, 0.7804, 0.7843,  ..., 0.1255, 0.1255, 0.1255],
          [0.7569, 0.7804, 0.7725,  ..., 0.1255, 0.1255, 0.1255],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.0941, 0.0667, 0.0980],
          [0.0745, 0.0824, 0.0902,  ..., 0.1020, 0.0863, 0.1059],
          [0.0745, 0.0824, 0.0863,  ..., 0.0824, 0.0824, 0.0824]],

         [[0.7569, 0.7961, 0.7961,  ..., 0.2863, 0.2863, 0.2863],
          [0.7529, 0.7882, 0.7882,  ..., 0.2863, 0.2863, 0.2863],
          [0.7647, 0.7843, 0.7765,  ..., 0.2863, 0.2863, 0.2863],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.1059, 0.0824, 0.1176],
          [0.0745, 0.0824, 0.0902,  ..., 0.1098, 0.0941, 0.1176],
          [0.0745, 0.0824, 0.0863,  ..., 0.0863, 0.0863, 0.0863]],

         [[0.7294, 0.7451, 0.7176,  ..., 0.7020, 0.7020, 0.7020],
          [0.7294, 0.7412, 0.7137,  ..., 0.7020, 0.7020, 0.7020],
          [0.7412, 0.7412, 0.7059,  ..., 0.7020, 0.7020, 0.7020],
          ...,
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.0941, 0.1294],
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.1059, 0.1255],
          [0.0824, 0.0902, 0.0941,  ..., 0.0902, 0.0941, 0.0941]]]],
       device='cuda:0'))
recoverd_img = torch.Size([2, 3, 256, 256])
images are saved in outputs/output_3.png
images are saved in outputs/z_4.png
images are saved in outputs/nosample_4.png
####cond finisihed #####
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps
ldm/modules/diffusionmodules/util.py, make_ddim_timsteps, Selected timesteps for ddim sampler: (200,)
ldm/modules/diffusionmodules/util.py, make_ddim_sampling_parameters, Selected alphas for ddim sampler: a_t: torch.Size([200]); a_(t-1): (200,)
For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)
ddim.py alphas_cumprod = tensor([0.9991, 0.9983, 0.9974, 0.9966, 0.9957, 0.9948, 0.9940, 0.9931, 0.9922,
        0.9913, 0.9904, 0.9895, 0.9886, 0.9877, 0.9868, 0.9859, 0.9850, 0.9841,
        0.9832, 0.9822, 0.9813, 0.9804, 0.9794, 0.9785, 0.9776, 0.9766, 0.9757,
        0.9747, 0.9737, 0.9728, 0.9718, 0.9708, 0.9698, 0.9689, 0.9679, 0.9669,
        0.9659, 0.9649, 0.9639, 0.9629, 0.9619, 0.9609, 0.9599, 0.9588, 0.9578,
        0.9568, 0.9557, 0.9547, 0.9537, 0.9526, 0.9516, 0.9505, 0.9495, 0.9484,
        0.9473, 0.9463, 0.9452, 0.9441, 0.9430, 0.9420, 0.9409, 0.9398, 0.9387,
        0.9376, 0.9365, 0.9354, 0.9343, 0.9332, 0.9320, 0.9309, 0.9298, 0.9287,
        0.9275, 0.9264, 0.9252, 0.9241, 0.9229, 0.9218, 0.9206, 0.9195, 0.9183,
        0.9171, 0.9160, 0.9148, 0.9136, 0.9124, 0.9112, 0.9100, 0.9089, 0.9077,
        0.9065, 0.9052, 0.9040, 0.9028, 0.9016, 0.9004, 0.8992, 0.8979, 0.8967,
        0.8955, 0.8942, 0.8930, 0.8917, 0.8905, 0.8892, 0.8880, 0.8867, 0.8854,
        0.8842, 0.8829, 0.8816, 0.8804, 0.8791, 0.8778, 0.8765, 0.8752, 0.8739,
        0.8726, 0.8713, 0.8700, 0.8687, 0.8674, 0.8661, 0.8647, 0.8634, 0.8621,
        0.8607, 0.8594, 0.8581, 0.8567, 0.8554, 0.8540, 0.8527, 0.8513, 0.8500,
        0.8486, 0.8473, 0.8459, 0.8445, 0.8431, 0.8418, 0.8404, 0.8390, 0.8376,
        0.8362, 0.8348, 0.8334, 0.8320, 0.8306, 0.8292, 0.8278, 0.8264, 0.8250,
        0.8236, 0.8221, 0.8207, 0.8193, 0.8179, 0.8164, 0.8150, 0.8136, 0.8121,
        0.8107, 0.8092, 0.8078, 0.8063, 0.8049, 0.8034, 0.8019, 0.8005, 0.7990,
        0.7975, 0.7960, 0.7946, 0.7931, 0.7916, 0.7901, 0.7886, 0.7871, 0.7856,
        0.7842, 0.7827, 0.7812, 0.7796, 0.7781, 0.7766, 0.7751, 0.7736, 0.7721,
        0.7706, 0.7690, 0.7675, 0.7660, 0.7645, 0.7629, 0.7614, 0.7599, 0.7583,
        0.7568, 0.7552, 0.7537, 0.7521, 0.7506, 0.7490, 0.7475, 0.7459, 0.7444,
        0.7428, 0.7412, 0.7397, 0.7381, 0.7365, 0.7350, 0.7334, 0.7318, 0.7302,
        0.7286, 0.7271, 0.7255, 0.7239, 0.7223, 0.7207, 0.7191, 0.7175, 0.7159,
        0.7143, 0.7127, 0.7111, 0.7095, 0.7079, 0.7063, 0.7047, 0.7031, 0.7015,
        0.6999, 0.6982, 0.6966, 0.6950, 0.6934, 0.6918, 0.6901, 0.6885, 0.6869,
        0.6852, 0.6836, 0.6820, 0.6803, 0.6787, 0.6771, 0.6754, 0.6738, 0.6722,
        0.6705, 0.6689, 0.6672, 0.6656, 0.6639, 0.6623, 0.6606, 0.6590, 0.6573,
        0.6557, 0.6540, 0.6524, 0.6507, 0.6490, 0.6474, 0.6457, 0.6441, 0.6424,
        0.6407, 0.6391, 0.6374, 0.6357, 0.6341, 0.6324, 0.6307, 0.6291, 0.6274,
        0.6257, 0.6241, 0.6224, 0.6207, 0.6190, 0.6174, 0.6157, 0.6140, 0.6123,
        0.6107, 0.6090, 0.6073, 0.6056, 0.6039, 0.6023, 0.6006, 0.5989, 0.5972,
        0.5955, 0.5939, 0.5922, 0.5905, 0.5888, 0.5871, 0.5855, 0.5838, 0.5821,
        0.5804, 0.5787, 0.5770, 0.5754, 0.5737, 0.5720, 0.5703, 0.5686, 0.5669,
        0.5652, 0.5636, 0.5619, 0.5602, 0.5585, 0.5568, 0.5551, 0.5535, 0.5518,
        0.5501, 0.5484, 0.5467, 0.5450, 0.5434, 0.5417, 0.5400, 0.5383, 0.5366,
        0.5350, 0.5333, 0.5316, 0.5299, 0.5282, 0.5266, 0.5249, 0.5232, 0.5215,
        0.5199, 0.5182, 0.5165, 0.5148, 0.5132, 0.5115, 0.5098, 0.5082, 0.5065,
        0.5048, 0.5032, 0.5015, 0.4998, 0.4982, 0.4965, 0.4948, 0.4932, 0.4915,
        0.4898, 0.4882, 0.4865, 0.4849, 0.4832, 0.4816, 0.4799, 0.4782, 0.4766,
        0.4749, 0.4733, 0.4716, 0.4700, 0.4684, 0.4667, 0.4651, 0.4634, 0.4618,
        0.4601, 0.4585, 0.4569, 0.4552, 0.4536, 0.4520, 0.4503, 0.4487, 0.4471,
        0.4455, 0.4438, 0.4422, 0.4406, 0.4390, 0.4374, 0.4357, 0.4341, 0.4325,
        0.4309, 0.4293, 0.4277, 0.4261, 0.4245, 0.4229, 0.4213, 0.4197, 0.4181,
        0.4165, 0.4149, 0.4133, 0.4117, 0.4101, 0.4086, 0.4070, 0.4054, 0.4038,
        0.4022, 0.4007, 0.3991, 0.3975, 0.3960, 0.3944, 0.3928, 0.3913, 0.3897,
        0.3882, 0.3866, 0.3850, 0.3835, 0.3819, 0.3804, 0.3789, 0.3773, 0.3758,
        0.3742, 0.3727, 0.3712, 0.3697, 0.3681, 0.3666, 0.3651, 0.3636, 0.3621,
        0.3605, 0.3590, 0.3575, 0.3560, 0.3545, 0.3530, 0.3515, 0.3500, 0.3485,
        0.3470, 0.3456, 0.3441, 0.3426, 0.3411, 0.3396, 0.3382, 0.3367, 0.3352,
        0.3338, 0.3323, 0.3308, 0.3294, 0.3279, 0.3265, 0.3250, 0.3236, 0.3222,
        0.3207, 0.3193, 0.3178, 0.3164, 0.3150, 0.3136, 0.3122, 0.3107, 0.3093,
        0.3079, 0.3065, 0.3051, 0.3037, 0.3023, 0.3009, 0.2995, 0.2981, 0.2967,
        0.2954, 0.2940, 0.2926, 0.2912, 0.2899, 0.2885, 0.2871, 0.2858, 0.2844,
        0.2831, 0.2817, 0.2804, 0.2790, 0.2777, 0.2763, 0.2750, 0.2737, 0.2723,
        0.2710, 0.2697, 0.2684, 0.2671, 0.2658, 0.2645, 0.2631, 0.2618, 0.2606,
        0.2593, 0.2580, 0.2567, 0.2554, 0.2541, 0.2528, 0.2516, 0.2503, 0.2490,
        0.2478, 0.2465, 0.2453, 0.2440, 0.2428, 0.2415, 0.2403, 0.2391, 0.2378,
        0.2366, 0.2354, 0.2341, 0.2329, 0.2317, 0.2305, 0.2293, 0.2281, 0.2269,
        0.2257, 0.2245, 0.2233, 0.2221, 0.2209, 0.2198, 0.2186, 0.2174, 0.2163,
        0.2151, 0.2139, 0.2128, 0.2116, 0.2105, 0.2093, 0.2082, 0.2071, 0.2059,
        0.2048, 0.2037, 0.2026, 0.2014, 0.2003, 0.1992, 0.1981, 0.1970, 0.1959,
        0.1948, 0.1937, 0.1926, 0.1915, 0.1905, 0.1894, 0.1883, 0.1872, 0.1862,
        0.1851, 0.1841, 0.1830, 0.1820, 0.1809, 0.1799, 0.1788, 0.1778, 0.1768,
        0.1757, 0.1747, 0.1737, 0.1727, 0.1717, 0.1707, 0.1696, 0.1686, 0.1677,
        0.1667, 0.1657, 0.1647, 0.1637, 0.1627, 0.1618, 0.1608, 0.1598, 0.1589,
        0.1579, 0.1569, 0.1560, 0.1550, 0.1541, 0.1532, 0.1522, 0.1513, 0.1504,
        0.1494, 0.1485, 0.1476, 0.1467, 0.1458, 0.1449, 0.1440, 0.1431, 0.1422,
        0.1413, 0.1404, 0.1395, 0.1386, 0.1378, 0.1369, 0.1360, 0.1352, 0.1343,
        0.1334, 0.1326, 0.1317, 0.1309, 0.1301, 0.1292, 0.1284, 0.1276, 0.1267,
        0.1259, 0.1251, 0.1243, 0.1235, 0.1227, 0.1219, 0.1211, 0.1203, 0.1195,
        0.1187, 0.1179, 0.1171, 0.1163, 0.1155, 0.1148, 0.1140, 0.1132, 0.1125,
        0.1117, 0.1110, 0.1102, 0.1095, 0.1087, 0.1080, 0.1073, 0.1065, 0.1058,
        0.1051, 0.1044, 0.1036, 0.1029, 0.1022, 0.1015, 0.1008, 0.1001, 0.0994,
        0.0987, 0.0980, 0.0973, 0.0967, 0.0960, 0.0953, 0.0946, 0.0940, 0.0933,
        0.0926, 0.0920, 0.0913, 0.0907, 0.0900, 0.0894, 0.0887, 0.0881, 0.0875,
        0.0868, 0.0862, 0.0856, 0.0850, 0.0844, 0.0837, 0.0831, 0.0825, 0.0819,
        0.0813, 0.0807, 0.0801, 0.0795, 0.0789, 0.0784, 0.0778, 0.0772, 0.0766,
        0.0761, 0.0755, 0.0749, 0.0744, 0.0738, 0.0732, 0.0727, 0.0721, 0.0716,
        0.0711, 0.0705, 0.0700, 0.0694, 0.0689, 0.0684, 0.0679, 0.0673, 0.0668,
        0.0663, 0.0658, 0.0653, 0.0648, 0.0643, 0.0638, 0.0633, 0.0628, 0.0623,
        0.0618, 0.0613, 0.0608, 0.0604, 0.0599, 0.0594, 0.0589, 0.0585, 0.0580,
        0.0575, 0.0571, 0.0566, 0.0562, 0.0557, 0.0553, 0.0548, 0.0544, 0.0539,
        0.0535, 0.0531, 0.0526, 0.0522, 0.0518, 0.0514, 0.0509, 0.0505, 0.0501,
        0.0497, 0.0493, 0.0489, 0.0485, 0.0481, 0.0477, 0.0473, 0.0469, 0.0465,
        0.0461, 0.0457, 0.0453, 0.0450, 0.0446, 0.0442, 0.0438, 0.0435, 0.0431,
        0.0427, 0.0424, 0.0420, 0.0416, 0.0413, 0.0409, 0.0406, 0.0402, 0.0399,
        0.0395, 0.0392, 0.0389, 0.0385, 0.0382, 0.0379, 0.0375, 0.0372, 0.0369,
        0.0365, 0.0362, 0.0359, 0.0356, 0.0353, 0.0350, 0.0347, 0.0343, 0.0340,
        0.0337, 0.0334, 0.0331, 0.0328, 0.0325, 0.0323, 0.0320, 0.0317, 0.0314,
        0.0311, 0.0308, 0.0305, 0.0303, 0.0300, 0.0297, 0.0295, 0.0292, 0.0289,
        0.0286, 0.0284, 0.0281, 0.0279, 0.0276, 0.0274, 0.0271, 0.0268, 0.0266,
        0.0264, 0.0261, 0.0259, 0.0256, 0.0254, 0.0251, 0.0249, 0.0247, 0.0244,
        0.0242, 0.0240, 0.0237, 0.0235, 0.0233, 0.0231, 0.0229, 0.0226, 0.0224,
        0.0222, 0.0220, 0.0218, 0.0216, 0.0214, 0.0212, 0.0210, 0.0207, 0.0205,
        0.0203, 0.0201, 0.0200, 0.0198, 0.0196, 0.0194, 0.0192, 0.0190, 0.0188,
        0.0186, 0.0184, 0.0182, 0.0181, 0.0179, 0.0177, 0.0175, 0.0174, 0.0172,
        0.0170, 0.0168, 0.0167, 0.0165, 0.0163, 0.0162, 0.0160, 0.0158, 0.0157,
        0.0155, 0.0154, 0.0152, 0.0151, 0.0149, 0.0147, 0.0146, 0.0144, 0.0143,
        0.0142, 0.0140, 0.0139, 0.0137, 0.0136, 0.0134, 0.0133, 0.0132, 0.0130,
        0.0129, 0.0127, 0.0126, 0.0125, 0.0123, 0.0122, 0.0121, 0.0120, 0.0118,
        0.0117, 0.0116, 0.0115, 0.0113, 0.0112, 0.0111, 0.0110, 0.0109, 0.0107,
        0.0106, 0.0105, 0.0104, 0.0103, 0.0102, 0.0101, 0.0100, 0.0098, 0.0097,
        0.0096, 0.0095, 0.0094, 0.0093, 0.0092, 0.0091, 0.0090, 0.0089, 0.0088,
        0.0087, 0.0086, 0.0085, 0.0084, 0.0083, 0.0082, 0.0082, 0.0081, 0.0080,
        0.0079, 0.0078, 0.0077, 0.0076, 0.0075, 0.0074, 0.0074, 0.0073, 0.0072,
        0.0071, 0.0070, 0.0070, 0.0069, 0.0068, 0.0067, 0.0066, 0.0066, 0.0065,
        0.0064, 0.0063, 0.0063, 0.0062, 0.0061, 0.0061, 0.0060, 0.0059, 0.0058,
        0.0058, 0.0057, 0.0056, 0.0056, 0.0055, 0.0054, 0.0054, 0.0053, 0.0053,
        0.0052, 0.0051, 0.0051, 0.0050, 0.0049, 0.0049, 0.0048, 0.0048, 0.0047,
        0.0047], device='cuda:0')
alpha_bar_u = tensor([0.9360, 0.9314], device='cuda:0')
start_timesteps = tensor([65, 69], device='cuda:0')
d = torch.Size([2, 4, 32, 32])
LPIPS = (tensor([[[[ 0.4752,  0.6135,  0.4910,  ..., -0.0054,  0.0454,  0.1404],
          [ 0.6383,  0.7358,  0.4470,  ..., -0.0601, -0.0269,  0.0043],
          [ 0.8071,  0.8313,  0.4952,  ..., -0.0981, -0.0498, -0.0074],
          ...,
          [ 0.9255,  0.8657,  0.6475,  ...,  0.5688,  0.6002,  0.5703],
          [ 0.8300,  0.8201,  0.5985,  ...,  0.5712,  0.6126,  0.5837],
          [ 0.5750,  0.6567,  0.5857,  ...,  0.4963,  0.5181,  0.5484]],

         [[ 0.5447,  0.6360,  0.5578,  ...,  0.4810,  0.5010,  0.5136],
          [ 0.7047,  0.7481,  0.5908,  ...,  0.4651,  0.4575,  0.4390],
          [ 0.8432,  0.8252,  0.6830,  ...,  0.4507,  0.4680,  0.4916],
          ...,
          [ 0.9762,  0.8291,  0.5905,  ...,  0.6009,  0.6260,  0.5681],
          [ 0.8480,  0.8005,  0.5684,  ...,  0.6192,  0.6318,  0.6065],
          [ 0.4921,  0.6065,  0.5611,  ...,  0.5441,  0.5495,  0.5560]],

         [[ 0.4868,  0.6996,  0.7572,  ...,  0.9419,  0.9717,  0.8750],
          [ 0.7011,  0.7791,  0.7360,  ...,  0.9841,  1.0192,  0.9950],
          [ 0.8014,  0.8541,  0.7574,  ...,  0.9664,  0.9953,  1.0149],
          ...,
          [ 0.7939,  0.7378,  0.4125,  ...,  0.5781,  0.5807,  0.5158],
          [ 0.7258,  0.7271,  0.4242,  ...,  0.5669,  0.5399,  0.5063],
          [ 0.4497,  0.5875,  0.4491,  ...,  0.4913,  0.4727,  0.4316]]],


        [[[ 0.4377,  0.6797,  0.7487,  ...,  0.3642,  0.3582,  0.3914],
          [ 0.3796,  0.5780,  0.7731,  ...,  0.3603,  0.3626,  0.3392],
          [ 0.3778,  0.5876,  0.7574,  ...,  0.3617,  0.3753,  0.3636],
          ...,
          [-0.0593, -0.0446, -0.0581,  ..., -0.0917, -0.0599, -0.0139],
          [-0.0624, -0.0468, -0.0656,  ..., -0.0832, -0.1293, -0.0493],
          [-0.0810, -0.0989, -0.0731,  ..., -0.0981, -0.0336,  0.0333]],

         [[ 0.4543,  0.7189,  0.8052,  ...,  0.5351,  0.5356,  0.5646],
          [ 0.4444,  0.6684,  0.8718,  ...,  0.5374,  0.5228,  0.5106],
          [ 0.4618,  0.6832,  0.8642,  ...,  0.5151,  0.5337,  0.5383],
          ...,
          [-0.0742, -0.0954, -0.0915,  ...,  0.0300, -0.0037,  0.0180],
          [-0.0755, -0.0875, -0.1078,  ...,  0.0354, -0.0517, -0.0127],
          [-0.1182, -0.1461, -0.1228,  ..., -0.0124,  0.0258,  0.0145]],

         [[ 0.4269,  0.7407,  0.8387,  ...,  1.0314,  1.0566,  0.9667],
          [ 0.4746,  0.7006,  0.8487,  ...,  1.0394,  1.0630,  1.0500],
          [ 0.5029,  0.7121,  0.8127,  ...,  1.0292,  1.0451,  1.0570],
          ...,
          [-0.1696, -0.1519, -0.1525,  ...,  0.0953,  0.0777,  0.0425],
          [-0.1625, -0.1555, -0.1747,  ...,  0.0775,  0.0064, -0.0074],
          [-0.1763, -0.2050, -0.1857,  ...,  0.0172,  0.0565, -0.0244]]]],
       device='cuda:0'), tensor([[[[1.0000, 0.5294, 0.0000,  ..., 0.5137, 0.5137, 0.5137],
          [1.0000, 0.5294, 0.0000,  ..., 0.4863, 0.4941, 0.5098],
          [1.0000, 0.5294, 0.0000,  ..., 0.3098, 0.3333, 0.3725],
          ...,
          [1.0000, 0.7922, 0.5176,  ..., 0.6471, 0.6235, 0.5922],
          [1.0000, 0.8431, 0.5922,  ..., 0.6078, 0.6235, 0.5686],
          [1.0000, 0.8275, 0.6314,  ..., 0.6392, 0.6549, 0.6353]],

         [[1.0000, 0.7176, 0.3961,  ..., 0.6902, 0.6824, 0.6784],
          [1.0000, 0.7176, 0.3961,  ..., 0.6706, 0.6784, 0.6863],
          [1.0000, 0.7176, 0.3961,  ..., 0.5373, 0.5608, 0.5882],
          ...,
          [1.0000, 0.7725, 0.4784,  ..., 0.5961, 0.5725, 0.5451],
          [1.0000, 0.8235, 0.5529,  ..., 0.5608, 0.5725, 0.5216],
          [1.0000, 0.8118, 0.5882,  ..., 0.5882, 0.6039, 0.5882]],

         [[1.0000, 0.8863, 0.7608,  ..., 0.8745, 0.8627, 0.8510],
          [1.0000, 0.8902, 0.7686,  ..., 0.8588, 0.8627, 0.8706],
          [1.0000, 0.8902, 0.7686,  ..., 0.7843, 0.8039, 0.8314],
          ...,
          [1.0000, 0.7373, 0.4039,  ..., 0.5216, 0.4902, 0.4588],
          [1.0000, 0.7882, 0.4784,  ..., 0.4824, 0.4902, 0.4353],
          [1.0000, 0.7725, 0.5137,  ..., 0.5137, 0.5255, 0.5059]]],


        [[[0.7451, 0.7843, 0.7922,  ..., 0.1255, 0.1255, 0.1255],
          [0.7451, 0.7804, 0.7843,  ..., 0.1255, 0.1255, 0.1255],
          [0.7569, 0.7804, 0.7725,  ..., 0.1255, 0.1255, 0.1255],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.0941, 0.0667, 0.0980],
          [0.0745, 0.0824, 0.0902,  ..., 0.1020, 0.0863, 0.1059],
          [0.0745, 0.0824, 0.0863,  ..., 0.0824, 0.0824, 0.0824]],

         [[0.7569, 0.7961, 0.7961,  ..., 0.2863, 0.2863, 0.2863],
          [0.7529, 0.7882, 0.7882,  ..., 0.2863, 0.2863, 0.2863],
          [0.7647, 0.7843, 0.7765,  ..., 0.2863, 0.2863, 0.2863],
          ...,
          [0.0745, 0.0824, 0.0902,  ..., 0.1059, 0.0824, 0.1176],
          [0.0745, 0.0824, 0.0902,  ..., 0.1098, 0.0941, 0.1176],
          [0.0745, 0.0824, 0.0863,  ..., 0.0863, 0.0863, 0.0863]],

         [[0.7294, 0.7451, 0.7176,  ..., 0.7020, 0.7020, 0.7020],
          [0.7294, 0.7412, 0.7137,  ..., 0.7020, 0.7020, 0.7020],
          [0.7412, 0.7412, 0.7059,  ..., 0.7020, 0.7020, 0.7020],
          ...,
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.0941, 0.1294],
          [0.0824, 0.0902, 0.0980,  ..., 0.1176, 0.1059, 0.1255],
          [0.0824, 0.0902, 0.0941,  ..., 0.0902, 0.0941, 0.0941]]]],
       device='cuda:0'))
recoverd_img = torch.Size([2, 3, 256, 256])
images are saved in outputs/output_4.png
